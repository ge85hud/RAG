[
  {
    "content": "CodeRAG: Finding Relevant and Necessary Knowledge for\nRetrieval-Augmented Repository-Level Code Completion\nSheng Zhang1,\u2020, Yifan Ding1,\u2020, Shuquan Lian1, Shun Song2, Hui Li1,\u00a7\n1Key Laboratory of Multimedia Trusted Perception and Efficient Computing\nMinistry of Education of China, Xiamen University\n2Ant Group\n{sheng, dingyf, shuquanlian}@stu.xmu.edu.cn,songshun.ss@antgroup.com\nhui@xmu.edu.cn",
    "metadata": {
      "source": "sample.pdf",
      "section": "unknown",
      "chunk_id": 0
    }
  },
  {
    "content": "Abstract\nRepository-level code completion automati-\ncally predicts the unfinished code based on\nthe broader information from the repository.\nRecent strides in Code Large Language Mod-\nels (code LLMs) have spurred the develop-\nment of repository-level code completion meth-\nods, yielding promising results. Nevertheless,\nthey suffer from issues such as inappropriate\nquery construction, single-path code retrieval,\nand misalignment between code retriever and\ncode LLM. To address these problems, we in-\ntroduce CodeRAG, a framework tailored to\nidentify relevant and necessary knowledge for\nretrieval-augmented repository-level code com-\npletion. Its core components include log prob-\nability guided query construction, multi-path\ncode retrieval, and preference-aligned BESTFIT",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 1
    }
  },
  {
    "content": "ability guided query construction, multi-path\ncode retrieval, and preference-aligned BESTFIT\nreranking. Extensive experiments on bench-\nmarks ReccEval and CCEval demonstrate that\nCodeRAG significantly and consistently out-\nperforms state-of-the-art methods. The imple-\nmentation of CodeRAG is available at https:\n//github.com/KDEGroup/CodeRAG.\n1 Introduction\nRecent years have witnessed the remarkable suc-\ncess of Large Language Models (LLMs) in various\nareas (Zhao et al., 2023). As a branch of LLMs,\nCode Large Language Models (code LLMs), are\ntrained on massive code data, enabling them to\ncomprehend and generate code snippets, thus as-\nsisting programmers in coding tasks and boosting\ndevelopment efficiency (Nijkamp et al., 2023; Roz-\ni\u00e8re et al., 2023; Li et al., 2023).",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 2
    }
  },
  {
    "content": "development efficiency (Nijkamp et al., 2023; Roz-\ni\u00e8re et al., 2023; Li et al., 2023).\nA typical application of code LLMs is code com-\npletion, which automatically predicts the unfin-\nished code (Svyatkovskiy et al., 2019). Early code\ncompletion methods solely leveragecode context\n(i.e., information from the function or source code\n\u2020The first two authors contribute equally.\n\u00a7Hui Li is the corresponding authors.\nfile that the programmer is working on) (Li et al.,\n2018; Wang and Li, 2021). However, real-world\nsoftware source code generally consists of multiple\ncode files with complex interdependencies, which\nwere neglected by early methods. These code files\nare essential ingredients for programmers to con-\nsider when developing unfinished code and they",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 3
    }
  },
  {
    "content": "are essential ingredients for programmers to con-\nsider when developing unfinished code and they\nare typically organized as a source code repository.\nThus, a practical code completion tool should be\nrepository-leveland leverage both code context and\ninformation retrieved from the entire codebase to\nprovide more accurate and comprehensive code\nsuggestions (Zhang et al., 2023).\nSince repository-level code completion can bet-\nter facilitate collaborative development and soft-\nware maintenance, there is a surge of work in\nthis direction (Zhang et al., 2023; Liu et al.,\n2024; Cheng et al., 2024) and most of them con-\nsider applying Retrieval-Augmented Generation\n(RAG), a prevalent solution incorporating external\nknowledge to help LLMs generate more accurate",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 4
    }
  },
  {
    "content": "(RAG), a prevalent solution incorporating external\nknowledge to help LLMs generate more accurate\ntext (Gao et al., 2023). Based on the idea of RAG,\nthese methods retrieve relevant code knowledge\nfrom the entire repository as supplementary to code\ncontext when predicting the unfinished code.\nDespite the blossom of related approaches, they\nstill suffer from the following shortcomings:\n\u2022 P1: Inappropriate Query Construction.Previ-\nous approaches use either the last k lines before\nthe cursor position (Tan et al., 2024) or the last\nk lines together with the generated code from\ncode LLM (Zhang et al., 2023) as the query for\ncode retrieval and find relevant code knowledge\nto assist completion, causing information loss\nand introducing noises. For example, program-",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 5
    }
  },
  {
    "content": "to assist completion, causing information loss\nand introducing noises. For example, program-\nmers may define key variables and classes, or\nimport packages at the beginning of a file, which\nare essential for understanding and completing\nthe code accurately. If the last k lines contain ir-\nrelevant code, the retrieved code knowledge will\n1\narXiv:2509.16112v1  [cs.CL]  19 Sep 2025\nconfig = T5Config.from_pretrained()\nModel = \nT5ForCausalLM.from_pretrained()\n# build search engine\nname = \"some name\u201d\nsearcher = retriever(name) \nmodel = ModelProvider(args)\nmodel.select_model(some_args)\nconfig = AutoModelConfig.from_pretrained()\nmodel = \nAutoModelForCausalLM.from_pretrained()\nCode Knowledge 1\ndef retriever(engine_name):\n\u2018\u2018\u2018construct the retrieval engine\u2019\u2019\u2019\npass\nCode Knowledge 2",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 6
    }
  },
  {
    "content": "Code Knowledge 1\ndef retriever(engine_name):\n\u2018\u2018\u2018construct the retrieval engine\u2019\u2019\u2019\npass\nCode Knowledge 2\nclass ModelProvider:\ndef __init__(self):\ndef select_model(self):\nCode Knowledge 3\nSparse Retrieval\nDense Retrieval\nDataflow-guided \nRetrieval\nFigure 1: Examples of different code retrieval paths,\nwhere the gray text indicates the code to be generated.\nmislead code LLM to generate inaccurate code.\n\u2022 P2: Single-path Code Retrieval.Existing meth-\nods either model code as plain text and chuck\ncode to construct the knowledge base for later\nsparse/dense retrieval (Zhang et al., 2023; Wu\net al., 2024), or construct specific data structures\n(e.g., dataflow graph) representing code for later\nretrieval (Liu et al., 2024; Cheng et al., 2024).\nWhile each method has its unique advantage and",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 7
    }
  },
  {
    "content": "retrieval (Liu et al., 2024; Cheng et al., 2024).\nWhile each method has its unique advantage and\nmay apply to some code completion cases, nei-\nther of them can handle all completion cases\nwell. For instance, Fig. 1 depicts three code\ncompletion examples and each fits one retrieval\nmethod. Sparse retrieval is ideal when the query\nand code knowledge directly overlap. Dense re-\ntrieval is more appropriate when the query and\ncode knowledge are semantically related. In con-\ntrast, dataflow-guided retrieval facilitates addi-\ntional searches based on variable instantiation.\n\u2022 P3: Misalignment between Code Retriever\nand Code LLM.Similar to other RAG appli-\ncations (Jin et al., 2024), inconsistencies may\nexist between retrieved code knowledge and nec-\nessary knowledge for code LLM, due to the sep-",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 8
    }
  },
  {
    "content": "exist between retrieved code knowledge and nec-\nessary knowledge for code LLM, due to the sep-\narate training process and learning objective of\ncode retriever and code LLM. While this issue\nhas recently been studied in various works on\nRAG for question answering (Zhang et al., 2024;\nDong et al., 2024), it is still underexplored for\nrepository-level code completion.\nTo address these issues, we propose a new frame-\nwork CodeRAG for findingrelevantandnecessary\nknowledge in retrieval-augmented repository-level\ncode completion. Our contributions are:\n\u2022 To overcomeP1, instead of using last k lines,\nCodeRAG adopts log probability guided probing\nto construct retrieval query for code retrieval.\n\u2022 To addressP2, CodeRAG employs multi-path\ncode retrieval over the constructed code knowl-",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 9
    }
  },
  {
    "content": "\u2022 To addressP2, CodeRAG employs multi-path\ncode retrieval over the constructed code knowl-\nedge base to benefit from the unique advantage\nof eachcode-specificretrieval path.\n\u2022 To alleviateP3, CodeRAG adopts preference-\naligned BESTFITreranking to efficiently find\nnecessary code knowledge. The retrieved code\nknowledge is reranked via LLM reranker accord-\ning to code LLM\u2019s preference. To reduce the\nreranking overhead, we further distill the pref-\nerence of LLM reranker into a smaller reranker\nand use it to conduct reranking.\n\u2022 CodeRAG feeds the reranked code knowledge\ninto code LLM for repository-level code comple-\ntion. Experiments on benchmarks ReccEval and\nCCEval show that CodeRAG significantly and\nconsistently exceeds state-of-the-art methods.\n2 Our Method CodeRAG",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 10
    }
  },
  {
    "content": "CCEval show that CodeRAG significantly and\nconsistently exceeds state-of-the-art methods.\n2 Our Method CodeRAG\nAs depicted in Fig. 2,CodeRAG involves five parts:\ncode knowledge base construction (Sec. 2.1), re-\ntrieval query construction (Sec. 2.2), multi-path\ncode retrieval (Sec. 2.2.1), preference-aligned\nBESTFITcode reranking (Sec. 2.3) and retrieval-\naugmented repository-level code completion.\n2.1 Code Knowledge Base Construction\nConstructing the repository-level code knowledge\nbase involves parsing and processing the code in\nthe repository, transforming raw code into struc-\ntured knowledge to enable more efficient retrieval,\nunderstanding, and reuse.\nTo construct the knowledge base for retrieval,\ngeneral RAG methods typically segment the text",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 11
    }
  },
  {
    "content": "understanding, and reuse.\nTo construct the knowledge base for retrieval,\ngeneral RAG methods typically segment the text\ncorpus based on predefined rules, such as split-\nting by length or delimiters (Sarthi et al., 2024).\nHowever, applying these approaches to code data\ncompromises the structural integrity and leads to\nthe loss of pertinent information. For instance, di-\nviding a class arbitrarily may result in omitting\nessential class-related details.\nTo alleviate the above problem, we propose a\nsegmentation strategy tailored to the construction\nof code knowledge base. Specifically, we consider\nfour elements in constructing the code knowledge\nbase: functions, global variables, class variables,\nand class functions, as depicted in Fig. 3. For a",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 12
    }
  },
  {
    "content": "base: functions, global variables, class variables,\nand class functions, as depicted in Fig. 3. For a\ntarget code repository, we first extract the Abstract\nSyntax Tree (AST) of each code file. Then, we\nextract the four types of elements from ASTs. This\nway, the code repository can be transformed into\na structured knowledge base, including function\n2\n\u2026\nCode Files\nin the\nRepository\nASTs Code\nKnowledge Base\nCode Knowledge Base Construction\n1\nCurrent Code File\nCode Chunk 1\nCode Chunk 2\nTarget Chunk\nCode LLM\nCode Retrieval\nQuery r\n0.4 0.3s1\nRetrieval Query Construction via Log \nProbability Guided Probing\n\u2026\n\u2026 \u2026\ns2 0.30.20.4\n2 Multi-path Code Retrieval\nKeyword \nmatrix\nSparse\nRetrieval\n1\n0 1\n01\n0\nSim Score\nEncoder\nDense\nRetrieval\nDataflow \nGuided \nRetrieval\nvar.unlink\nvar = func()\ndef func():",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 13
    }
  },
  {
    "content": "Retrieval\n1\n0 1\n01\n0\nSim Score\nEncoder\nDense\nRetrieval\nDataflow \nGuided \nRetrieval\nvar.unlink\nvar = func()\ndef func():\npass\n3\nPreference-Aligned BestFit Reranking4\nBestFit\nPrompting\nDistilled\nReranker\nrerank\nRetrieval-Augmented\nCode Completion\n5\n0 11\n0.10.10.2\nCode\nKnowledge 1\nCode\nKnowledge n\nCode\nKnowledge 4\n\u2026 Code\nKnowledge 2\nLLM\nReranker\nCode LLM\nFigure 2: Overview of CodeRAG.\n1 Functions\ndef func(*args):\npass\n2 Variables VAR_CONST = \u201ctest\u201d\n3 Cls Funcs class myClass:\ndef func(self)\n4 Cls Vars class myClass:\nself.name = \u201c\u201d\nElement Knowledge\nCode Block 1\nCode Block 2\nCode Block 3\nUnfinished\nFigure 3: Examples of Code Knowledge Base Items.\ncalls and variable usage, providing data support for\nrepository-level code completion.\n2.2 Retrieval Query Construction via Log",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 14
    }
  },
  {
    "content": "repository-level code completion.\n2.2 Retrieval Query Construction via Log\nProbability Guided Probing\nIn standard RAG, a retrieval query conveys the\nuser intent from the user query or consists of a\nspecific text chunk from a document. Using the\nretrieval query, the RAG framework can retrieve\nrelevant knowledge from the knowledge base to\nassist text generation. In existing repository-level\ncode completion methods, the concept of retrieval\nquery shifts to represent an incomplete code seg-\nment (Zhang et al., 2023), which could be an un-\nfinished function, a partially defined variable, or a\nmethod call within a class (i.e., code context).\nTo overcome the limitation of using the last k\nlines as the code retrieval query (i.e.,P1illustrated\nAlgorithm 1:Construct Retrieval Query",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 15
    }
  },
  {
    "content": "lines as the code retrieval query (i.e.,P1illustrated\nAlgorithm 1:Construct Retrieval Query\nInput:C(code file to be completed),f(chunk\nlength),m(number of generation step),g\n(number of selected chunks)\nOutput:r(code retrieval query)\nFunctionQueryConstruction(C,f,m,g):\nDivideCinto fine-grained chunks and each\nchunk havingflines.\nforeach chunkc i do\nifc i is the target chunkthen\nPass.\nConcatenatec i to the target chunk.\nFeed the concatenation into code LLM to\ngeneratemnew tokens.\nRecord the highest log probability for all\ntokens in the vocabulary at each generation\nstep.\nSummprobability scores as the confidence\nscores i.\nSelect the top-gchunks with the highest\nconfidence scoress.\nConcatenate thegchunks with the target chunk\nas the retrieval queryr.\nreturnr.",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 16
    }
  },
  {
    "content": "confidence scoress.\nConcatenate thegchunks with the target chunk\nas the retrieval queryr.\nreturnr.\nin Sec. 1), we propose to construct the code re-\ntrieval query based on the log probability gain.\nAlg. 1 depicts the overall procedure for code re-\ntrieval query construction. The core idea is to use\nlog probability to find the fine-grained code chunks\nthat are most important to constructing code re-\ntrieval query. In repository-level code completion,\nwe can view log probability as the confidence of\ncode LLMs, given the code retrieval query. In other\n3\nwords, log probability can reveal the relevance of\nthe code chunks in the retrieval query.\nAs shown in Alg. 1, we first chunk the code\nfile that the programmer is working on into fine-\ngrained pieces and each of them contains f lines.",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 17
    }
  },
  {
    "content": "file that the programmer is working on into fine-\ngrained pieces and each of them contains f lines.\nThen, we concatenate each fine-grained chunk to\nthe chunk containing unfinished code (the target\nchunk) as the probe and feed it to code LLM to\ngenerate m tokens. For simplicity, we choose the\ntoken with the maximum log probability at each\nstep and use CodeT5p-220m1 as code LLM for this\nstep. The sum of the log probabilities for all gen-\nerated token is recorded as the relevance score for\nthe fine-grained chunk corresponding to the probe.\nFinally, the top-g fine-grained chunks with the high-\nest relevance scores are concatenated together with\nthe target chunk as the retrieval query.\n2.2.1 Multi-path Code Retrieval\nAs an essential part of RAG, code retriever finds rel-",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 18
    }
  },
  {
    "content": "2.2.1 Multi-path Code Retrieval\nAs an essential part of RAG, code retriever finds rel-\nevant code knowledge from code knowledge base\naccording to the code retrieval query. Early code\nretrieval methods rely on traditional information\nretrieval methods like TF-IDF and BM25 (sparse re-\ntrieval), and recent code retrieval approaches com-\nmonly adopt embedding based methods (Dense\nRetrieval) (Sun et al., 2024). Most recently, Cheng\net al. (2024) find that dataflow can also be used\nto guide code retrieval (dataflow-guided retrieval).\nThese methods consider code retrieval from a sin-\ngle perspective and retrieve word-matching knowl-\nedge, semantically relevant knowledge, or knowl-\nedge having data dependency relations with the tar-\nget chunk. Each of them has its unique advantages",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 19
    }
  },
  {
    "content": "edge having data dependency relations with the tar-\nget chunk. Each of them has its unique advantages\nand can well provide retrieved code knowledge for\nsome code completion cases. Hence, we argue that\nconducting a multi-path code retrieval can better of-\nfer code knowledge for later code completion. Our\ndesigned multi-path code retrieval step involves the\nfollowing three code retrieval paths:\nSparse Retrieval. Sparse retrieval relies on key-\nword matching between the retrieval query and\ncode knowledge in the code knowledge base, which\nidentifies exact or closely related keywords within\nthe codebase, to obtain relevant invocation details,\nAPI calls, and code snippets that share similar struc-\ntures or functionality. Sparse retrieval is efficient",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 20
    }
  },
  {
    "content": "API calls, and code snippets that share similar struc-\ntures or functionality. Sparse retrieval is efficient\nand particularly effective when searching syntacti-\ncally similar code or commonly used functions, as\n1https://huggingface.co/Salesforce/\ncodet5p-220m\nit can quickly pinpoint segments that contain spe-\ncific terms or identifiers. We use TF-IDF (Jones,\n2004) for sparse retrieval.\nDense Retrieval.Dense retrieval leverages an en-\ncoding model to encode the retrieval query and\ncode knowledge in the code knowledge base into\nrepresentations. The query is encoded at the chunk\nlevel, whereas the items in the knowledge base are\neither at the function level (functions) or the line\nlevel (variables). Code knowledge that has high\nsimilarity with the retrieval query w.r.t. their rep-",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 21
    }
  },
  {
    "content": "level (variables). Code knowledge that has high\nsimilarity with the retrieval query w.r.t. their rep-\nresentations is retrieved. We use cosine similarity\nas the similarity measure and adopt the pre-trained\nencoder in CodeT5p-220m as the encoding model.\nDataflow-Guided Retrieval. It finds relevant in-\nformation w.r.t. the target chunk in the current code\nfile according to data dependency relations. Fol-\nlowing Cheng et al. (2024), we first formulate the\nunfinished code file into a dataflow graph. Once\nthe graph is built, we can retrieve the dependency\nstarting from the last unfinished line in the dataflow\ngraph as the retrieved code knowledge.\nFor sparse and dense retrieval, we use the con-\nstructed retrieval query to retrieve j results from",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 22
    }
  },
  {
    "content": "For sparse and dense retrieval, we use the con-\nstructed retrieval query to retrieve j results from\neach path. If data dependency exists in the dataflow\ngraph, we retrieve dependency-related code via\ndataflow-guided retrieval. After receiving all re-\ntrieved results from three paths, we add them to a\nretrieval list containingn(i.e.,2j+ 1) results.\n2.3 Find Necessary Code Knowledge through\nPreference-Aligned BESTFITReranking\nThe retrieved code knowledge is used to augment\nthe code completion prompt, directly affecting the\nquality of code completion. Solely using the multi-\npath code retriever may not provide an appropriate\norder of relevant knowledge. The reason is the\nmisalignment between the code retriever and code\nLLM, which is caused by their separate training ob-",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 23
    }
  },
  {
    "content": "misalignment between the code retriever and code\nLLM, which is caused by their separate training ob-\njectives (Zhang et al., 2024). Therefore, we further\ndeploy a reranking module that reranks retrieved\ncode knowledge according to code LLM\u2019s prefer-\nence and only keep top-u code knowledge (u < n).\n2.3.1 BESTFITCode Reranking\nTo address the misalignment, a natural way is to\ntrain the reranker using feedback signals from code\nLLM. However, in repository-level code comple-\ntion, it is very difficult to acquire feedback from\ncode LLM that can perfectly show the quality of\nthe generated code. One possible solution is ap-\n4\nWhich of the retrieved code snippets is most\nhelpful for completing the following code snippet?\nThe code snippet to be completed:\n{query}\nThe retrieved code snippet(s):",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 24
    }
  },
  {
    "content": "The code snippet to be completed:\n{query}\nThe retrieved code snippet(s):\n{code snippets}\nPlease provide only the label of the most helpful\nretrieved code snippet, enclosed in square\nbrackets, within the answer tags. For example, if\nthe code snippet C is the most helpful, the\nanswer should be: <answer>[C]</answer>\nFigure 4: Prompt used for LLM-based BESTFITcode\nreranking.\nplying unit tests on the generated code from code\nLLM (Ma et al., 2025). While conducting unit tests\nis possible for function-level code completion, it\nis costly in the repository-level setting where the\ncomplete project must be executed in order to see\nthe impact of inserting generated code. Besides,\nunlike function-level code completion where inputs\nand outputs to unit test are easy to design, craft-",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 25
    }
  },
  {
    "content": "unlike function-level code completion where inputs\nand outputs to unit test are easy to design, craft-\ning inputs and labeling outputs to unit tests in the\nrepository-level setting is much harder (e.g., more\nexecution parameters or outputs are not variables).\nConsidering the above difficulty, an alternative\nis to apply an LLM as a zero-shot reranker (Sun\net al., 2023). And the LLM is instructed to di-\nrectly produce the reranking list of the retrieval\ncode knowledge pieces according to their relevance\nto the query. Although recent studies (Sun et al.,\n2023; Pradeep et al., 2023) have shown the strong\nability of LLMs on zero-shot document reranking,\nwe empirically find that this listwise prompting so-\nlution does not work well on reranking code knowl-",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 26
    }
  },
  {
    "content": "we empirically find that this listwise prompting so-\nlution does not work well on reranking code knowl-\nedge: (1) LLMs with a few billion parameters that\ncan be deployed locally more easily do not strictly\nadhere to listwise prompting, while calling APIs of\nonline LLMs that have much larger model sizes and\ncan understand and strictly follow listwise prompt-\ning incurs high overhead. (2) Listwise prompting\nitself is computationally intensive since the rerank-\ning list is generated token by token. LLM reranker\nmust do one inference for each next token predic-\ntion during reranking list generation.\nTo overcome this issue, we propose BESTFIT\ncode reranking that prompts the LLM reranker to\npick the most relevant code knowledge from the\nretrieval list to the query. The prompt is listed",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 27
    }
  },
  {
    "content": "pick the most relevant code knowledge from the\nretrieval list to the query. The prompt is listed\nin Fig. 4. This way, the inference cost is signifi-\n\u2026 rank code knowledge\n\u2026 which code knowledge\nis most relevant \u2026\n\u2026\nRetrieved\nCode\nLLM\nReranker\n[Index 1]\u2026[Index u]\n[Index x]\nListwise Prompting\nBestFit Prompting",
    "metadata": {
      "source": "sample.pdf",
      "section": "abstract",
      "chunk_id": 28
    }
  },
  {
    "content": "Results\nFigure 5: A comparison between listwise code reranking\nand BESTFITcode reranking.\nFigure 6: Heap sort operation finally moves top-u code\nknowledge pieces to the top. Each circle denotes a\nwindow of 3 code knowledge pieces.\ncantly reduced as we only need a single forward\npass of the LLM reranker. Fig. 5 depicts the differ-\nence between listwise and BESTFITcode reranking.\nMoreover, we find that an LLM with a few billion\nparameters can strictly follow BESTFITprompt-\ning. Hence, we directly use Qwen3-8B as LLM\nreranker2, avoiding additional instructing tuning of\nLLM reranker or calling online LLM APIs.\nTo avoid exceeding LLM\u2019s input length, we im-\nplement a sliding window strategy that divides the\nretrieval list into several equal-sized windows, and",
    "metadata": {
      "source": "sample.pdf",
      "section": "results",
      "chunk_id": 29
    }
  },
  {
    "content": "plement a sliding window strategy that divides the\nretrieval list into several equal-sized windows, and\nthe adjacent two windows share one code knowl-\nedge. Fig. 6 provides an example with a window\nsize of 3. Each time, we feed one window to LLM\nreranker and ask it to pick only the most helpful\ncode knowledge. Inspired by prior work (Qin et al.,\n2024; Zhuang et al., 2024) that uses sorting algo-\nrithms to speed up LLM-based pairwise reranking,\nwe apply heap sort to accelerate BESTFITcode\nreranking. Windows are organized as a heap tree\nand each time we use LLM reranker as the com-\nparator to find the most relevant code in a window.\nHeap sort can quickly find the top-u most relevant\ncode knowledge in the reranking list. We choose\nheap sort instead of other sorting methods due to",
    "metadata": {
      "source": "sample.pdf",
      "section": "results",
      "chunk_id": 30
    }
  },
  {
    "content": "code knowledge in the reranking list. We choose\nheap sort instead of other sorting methods due to\nits simplicity and the complexityO(NlogN).\n2.3.2 Distilled Reranker\nEven though BESTFITcode reranking only re-\nquires the LLM reranker to have a few billion pa-\nrameters, directly employing the LLM reranker\n2https://huggingface.co/Qwen/Qwen3-8B\n5\nAlgorithm 2:Construct Distillation Data\nInput:r(code retrieval query),L\n(initial retrieval list forr),N(sample\nnumbers)\nOutput:S(distillation training sample forr)\nFunctionDataConstruction(r,L):\nforiinNdo\nforj= 1to3do\nRandomly pickicode knowledge from\nLas the retrieved code snippet(s)\n{code snippets} in Fig. 4.\nC \u2190[ ]\nforz= 1to5do\nUse BESTFITreranking prompt in\nFig. 4 to guide LLM reranker to\nselect [C] from {code snippets}.\nAdd [C] toC.",
    "metadata": {
      "source": "sample.pdf",
      "section": "results",
      "chunk_id": 31
    }
  },
  {
    "content": "Use BESTFITreranking prompt in\nFig. 4 to guide LLM reranker to\nselect [C] from {code snippets}.\nAdd [C] toC.\nifOne code knowledge [C] occurs at\nleast four times inCthen\nAdd{r,{code snippets},[C]}to\nS.\nreturnS.\nmay still incur a high computational cost. Hence,\nwe distill the preference of the LLM reranker into\na much smaller reranker model.\nTo train the distilled reranker, we first use a data\naugmentation strategy (Alg. 2) to construct distil-\nlation training data. We use unfinished code in the\ntraining data to formulate code retrieval queries and\nconduct multi-path code retrieval to produce initial\nretrieval lists. Then, we conduct data augmentation\nby generating multiple variations of each initial re-\ntrieval list L, where each variation contains differ-",
    "metadata": {
      "source": "sample.pdf",
      "section": "results",
      "chunk_id": 32
    }
  },
  {
    "content": "by generating multiple variations of each initial re-\ntrieval list L, where each variation contains differ-\ning amounts of code knowledge. After that, we use\nLLM reranker with BESTFITprompting to select\nthe most helpful code knowledge from each gener-\nated variation, repeating this process five times to\nassess selection consistency. When LLM reranker\ndemonstrates high confidence in its selections (i.e.,\nwhen consistent choices appear in at least four out\nof five trials), the generated list, together with the\ncorresponding code retrieval query and consensus\nselection, is treated as a distillation training sam-\nple S. This way, S reflects LLM reranker\u2019s most\nreliable decision patterns. We use all curated distil-\nlation samples to fine-tune Qwen3-0.6B3, the back-",
    "metadata": {
      "source": "sample.pdf",
      "section": "results",
      "chunk_id": 33
    }
  },
  {
    "content": "reliable decision patterns. We use all curated distil-\nlation samples to fine-tune Qwen3-0.6B3, the back-\nbone of the distilled reranker, using LoRA (Hu\net al., 2022) and token-level cross-entropy loss.\nFinally, the trained distilled reranker is used\nin CodeRAG to actually rerank the retrieved\ncode knowledge and CodeRAG retains top-u code\nknowledge in the reranking list (u < n).\n3https://huggingface.co/Qwen/Qwen3-0.6B\n2.4 Putting All Together\nDuring completion, for an unfinished code file,\nCodeRAG firstly constructs the corresponding\ncode retrieval query r. Then CodeRAG uses r to\nconduct multi-path code retrieval over the code\nknowledge base and retrieves top-n relevant code\nknowledge. After that, CodeRAG leverages the\nBESTFITreranker to rank the n relevant code",
    "metadata": {
      "source": "sample.pdf",
      "section": "results",
      "chunk_id": 34
    }
  },
  {
    "content": "knowledge. After that, CodeRAG leverages the\nBESTFITreranker to rank the n relevant code\nknowledge and retains the top- u necessary code\nknowledge pieces. Finally, the code context of\nthe unfinished code file is concatenated with theu\npieces of code knowledge, and the result is fed into\ncode LLM to generate the completion.\n3 Experiment\n3.1 Evaluation Settings\nMetrics.We use prevalent metrics (Liu et al., 2024;\nCheng et al., 2024; Zhang et al., 2023) to evaluate\nwhether the generated code matches the ground-\ntruth code (code match) and whether the identifiers\nin the generated code match the ground-truth iden-\ntifiers (identifier match):\n\u2022 Code Match.Exact Match (EM) and Edit Sim-\nilarity (ES)4 are employed to assess code align-\nment. EM is 1 when the generated code is identi-",
    "metadata": {
      "source": "sample.pdf",
      "section": "results",
      "chunk_id": 35
    }
  },
  {
    "content": "ilarity (ES)4 are employed to assess code align-\nment. EM is 1 when the generated code is identi-\ncal to the ground-truth answer, and 0 otherwise.\nES provides a more nuanced evaluation, calcu-\nlated as ES= 1\u2212Lev(x, y)/max(\u2225x\u2225,\u2225y\u2225) ,\nwhere Lev(\u00b7)denotes the Levenshtein distance.\n\u2022 Identifier Match. We utilize EM and F1 scores\nto evaluate the alignment of identifiers in the\ngenerated code and the ground-truth answer.\nBaselines.We use the following representative\nrepository-level code completion baselines:\n\u2022 Zero-Shot5. Zero-Shot directly feeds the com-\npleted code before the cursor position into code\nLLM without utilizing any repository informa-\ntion, which can reveal code LLM\u2019s basic ability\nto complete the code. Following existing works,\nwe directly feed the completed code into code",
    "metadata": {
      "source": "sample.pdf",
      "section": "results",
      "chunk_id": 36
    }
  },
  {
    "content": "to complete the code. Following existing works,\nwe directly feed the completed code into code\nLLM without further using a prompt.\n\u2022 CCFinder6 (Ding et al., 2024). CCFinder is a\nretrieval tool for searching repository contexts\n4Note that the paper of DraCo adopts a different way\nto measure ES (see https://github.com/nju-websoft/\nDraCo?tab=readme-ov-file#evaluation). We follow the\ndefinition of ES used in the paper of RepoCoder.\n5https://huggingface.co/Salesforce/\ncodegen-350M-mono\n6https://github.com/amazon-science/cocomic\n6\nfor the incomplete file. Based on import state-\nments, it retrieves 2-hop corresponding contexts\nto augment completion.\n\u2022 RG-17 (Zhang et al., 2023). RG-1 is the standard\nRAG pipeline, which contains a Bag of Word\nretriever (Salton et al., 1975) (sparse retrieval)",
    "metadata": {
      "source": "sample.pdf",
      "section": "results",
      "chunk_id": 37
    }
  },
  {
    "content": "RAG pipeline, which contains a Bag of Word\nretriever (Salton et al., 1975) (sparse retrieval)\nto retrieve the code knowledge from the code\nknowledge base using the unfinished code as\nthe query. The code knowledge base consists of\nequal-length code chunks.\n\u2022 RepoCoder7 (Zhang et al., 2023). RepoCoder\ngenerates code iteratively, leveraging the genera-\ntion in the previous cycle to retrieve information\nfrom the code knowledge base and enhance the\nnext generation. The code knowledge base con-\nsists of equal-length code chunks.\n\u2022 DraCo8 (Cheng et al., 2024). DraCo extracts\ncode entities and their relations through dataflow\nanalysis, forming a repository-specific context\ngraph. During completion, it searches the graph\nto retrieve code features to enhance generation.",
    "metadata": {
      "source": "sample.pdf",
      "section": "results",
      "chunk_id": 38
    }
  },
  {
    "content": "graph. During completion, it searches the graph\nto retrieve code features to enhance generation.\n\u2022 RepoFuse9 (Liang et al., 2024). Repo-\nFuse (Liang et al., 2024) fuses analogy con-\ntext and rationale context (sparse retrieval using\nBM25) using a code LM to score and choose the\nmost similar chunks to the target chunk to con-\nstruct the code completion prompt. We choose\nUniXcoder (Guo et al., 2022) as the scoring mod-\nule as it shows the best result in the paper.\n\u2022 Repoformer-3B10 (Wu et al., 2024).\nRepoformer-3B is a code LLM that can\nself-evaluate whether retrieval is necessary. It\nacts as both the selective RAG policy and the\ngeneration model.Note that its definition of\ncode completion differs from other baselines and\nCodeRAG since it assumes both the left part and",
    "metadata": {
      "source": "sample.pdf",
      "section": "results",
      "chunk_id": 39
    }
  },
  {
    "content": "code completion differs from other baselines and\nCodeRAG since it assumes both the left part and\nthe right part of cursor position are contexts.\nCode LLMs.We use four representative code\nLLMs with different parameter numbers ranging\nfrom 350M to 7B as the code generators for all\nbaselines and CodeRAG: CodeGen-350M 5 (Ni-\njkamp et al., 2023), SantaCoder-1.1B11 (Allal et al.,\n2023), StarCoder2-3B 12 (Lozhkov et al., 2024),\n7https://github.com/microsoft/CodeT/tree/main/\nRepoCoder\n8https://github.com/nju-websoft/DraCo\n9https://github.com/codefuse-ai/RepoFuse\n10https://huggingface.co/xiaowu0162/\nrepoformer-3b\n11https://huggingface.co/bigcode/santacoder\n12https://huggingface.co/bigcode/starcoder2-3b\nand Qwen2.5-Coder-7B13 (Hui et al., 2024).\nDatasets.We use two benchmarks for evaluation:",
    "metadata": {
      "source": "sample.pdf",
      "section": "results",
      "chunk_id": 40
    }
  },
  {
    "content": "and Qwen2.5-Coder-7B13 (Hui et al., 2024).\nDatasets.We use two benchmarks for evaluation:\n\u2022 ReccEval14 (Cheng et al., 2024). ReccEval con-\ntains 6,461 test cases. It focuses on Python and\ncontains projects first released on PyPI between\n2023-01-01 and 2023-04-28. It is released under\nGPL-3.0 license.\n\u2022 CCEval15 (Ding et al., 2023). CCEval is a multi-\nlingual benchmark for repository-level code com-\npletion, where the statement to be completed has\nat least one use of cross-file API. CCEval con-\ntains 2,665 test cases. We conduct experiments\non the Python subset. CCEval is released under\nApache-2.0 license.\nEnvironment and Hyper-Parameters.We use a\nmachine with two Intel(R) Xeon(R) Silver 4314\nCPU @ 2.40GHz and one NVIDIA A800 GPU for",
    "metadata": {
      "source": "sample.pdf",
      "section": "results",
      "chunk_id": 41
    }
  },
  {
    "content": "experiments. The maximum number of generation\ntokens is set to 48. The temperature during gen-\neration is set to 0. The maximum input length of\nall code LLMs is set to 2,048 by default. We use\nthe Text Generation Inference16 framework to ac-\ncelerate LLM inference. By default, we use LLM\nreranker in CodeRAG. We set f and g to 3 and 1\nin Alg. 1, respectively. We setj to 15 in multi-path\ncode retrieval and u to 10 in reranking. We use\nN={2,3,4,5,6,7}in Alg. 2.\n3.2 Evaluation Results 17\n3.2.1 Overall Performance\nWe first report the results when using all ReccEval\ndata for evaluation in Tab. 1 and Tab. 2, as done\nin existing works. In this case, LLM reranker (i.e.,\nCodeRAGllmr) is used instead of distilled reranker\n(i.e., CodeRAGdisr) since distilled reranker requires",
    "metadata": {
      "source": "sample.pdf",
      "section": "experiments",
      "chunk_id": 42
    }
  },
  {
    "content": "CodeRAGllmr) is used instead of distilled reranker\n(i.e., CodeRAGdisr) since distilled reranker requires\ntraining data. The results of splitting ReccEval for\ntraining and evaluating the distilled reranker are\nanalyzed in Sec. 3.2.3. From Tab. 1 and Tab. 2, we\nhave the following observations:\n\u2022 It is clear that Zero-Shot has the worst perfor-\nmance across all settings, showing that solely\nrelying on code LLMs cannot provide satisfying\nrepository-level code completion.\n\u2022 CCFinder and RG-1 show significant improve-\nments over Zero-Shot but they are outperformed\n13https://huggingface.co/Qwen/Qwen2.5-Coder-7B\n14https://github.com/nju-websoft/DraCo\n15https://github.com/amazon-science/cceval\n16https://huggingface.co/docs/\ntext-generation-inference/index",
    "metadata": {
      "source": "sample.pdf",
      "section": "experiments",
      "chunk_id": 43
    }
  },
  {
    "content": "15https://github.com/amazon-science/cceval\n16https://huggingface.co/docs/\ntext-generation-inference/index\n17The results for CCEval are reported in Appendix A.\n7\nTable 1: Performance on ReccEval (Use 100% data for evaluation). Bold and underlined values indicate the best\nand the second-best results, respectively.",
    "metadata": {
      "source": "sample.pdf",
      "section": "experiments",
      "chunk_id": 44
    }
  },
  {
    "content": "Methods\nCodeGen-350M SantaCoder-1.1B StarCoder2-3B Qwen2.5-Coder-7B\nCode Match Identifier Match Code Match Identifier Match Code Match Identifier Match Code Match Identifier Match\nEM ES EM F1 EM ES EM F1 EM ES EM F1 EM ES EM F1\nZero-Shot 2.70 43.02 8.26 37.85 4.35 46.52 10.58 41.88 6.53 48.56 12.91 44.52 11.11 52.19 18.09 48.35\nCCFinder 10.58 48.65 17.19 45.96 14.63 53.44 23.08 51.90 21.08 58.11 29.42 57.46 24.80 59.52 33.47 62.61\nRG-1 8.78 49.54 16.47 46.33 12.83 53.78 21.99 51.76 17.78 57.74 27.35 56.55 22.51 61.84 32.91 60.68\nRepoCoder 10.58 51.0719.06 48.93 15.12 55.66 24.62 53.78 21.24 60.90 31.56 60.00 25.89 63.65 36.21 63.09\nDraCo 12.83 50.71 20.3348.91 19.7057.1729.04 56.82 26.6862.1136.29 62.75 30.6965.4640.64 66.26",
    "metadata": {
      "source": "sample.pdf",
      "section": "methods",
      "chunk_id": 45
    }
  },
  {
    "content": "DraCo 12.83 50.71 20.3348.91 19.7057.1729.04 56.82 26.6862.1136.29 62.75 30.6965.4640.64 66.26\nRepoFuse 11.22 50.78 19.29 48.77 17.64 56.52 27.02 55.65 23.34 61.28 33.43 61.16 27.73 64.76 38.39 64.82\nCodeRAGllmr 14.11 52.44 22.28 51.56 22.89 59.92 32.42 60.24 30.66 65.46 41.13 66.62 35.20 68.93 45.97 70.47\nTable 9: Performance on CCEval (Use 30% data for evaluation). Bold and underlined values indicate the best and\nthe second-best results, respectively.\nCodeGen-350M SantaCoder-1.1B StarCoder2-3B Qwen2.5-Coder-7B\nCode Match Identifier Match Code Match Identifier Match Code Match Identifier Match Code Match Identifier Match\nEM ES EM F1 EM ES EM F1 EM ES EM F1 EM ES EM F1\nZero-Shot 2.57 41.34 6.77 35.29 3.88 45.72 9.41 40.11 6.88 47.45 11.79 42.26 9.65 50.89 15.29 45.10",
    "metadata": {
      "source": "sample.pdf",
      "section": "methods",
      "chunk_id": 46
    }
  },
  {
    "content": "Zero-Shot 2.57 41.34 6.77 35.29 3.88 45.72 9.41 40.11 6.88 47.45 11.79 42.26 9.65 50.89 15.29 45.10\nCCFinder 9.33 46.20 14.24 42.69 13.41 52.06 21.18 50.08 20.30 56.07 25.90 53.32 23.53 58.82 30.00 59.85\nRG-1 8.52 48.11 15.52 43.90 11.76 53.03 20.24 49.37 16.57 54.80 23.80 51.89 21.53 60.49 31.06 57.98\nRepoCoder 10.62 49.4917.39 46.22 14.59 54.87 23.06 51.74 21.24 58.46 28.47 56.28 24.71 62.35 33.88 60.57\nDraCo 11.55 47.71 17.04 44.95 19.6055.62 26.8453.52 25.6759.59 32.32 58.6829.9963.8836.17 63.86\nRepoFuse 11.4149.87 18.82 46.4018.35 56.0026.59 54.00 23.0661.2732.5959.56 26.71 63.36 35.88 62.08\nCodeRAGdisr 11.7848.39 17.7445.04 20.07 56.30 27.77 54.75 26.2560.5333.49 60.2128.7064.56 36.5263.76\nTable 10: Results of RepoFormer-3B on CCEval (Use",
    "metadata": {
      "source": "sample.pdf",
      "section": "methods",
      "chunk_id": 47
    }
  },
  {
    "content": "Table 10: Results of RepoFormer-3B on CCEval (Use\n100% Data for evaluation). l: only use left context. lr:\nuse left and right contexts.\nDataset\nRepoFormer-3B\nCode Match Identifier Match\nEM ES EM F1\nCCEvall 8.18 50.19 15.53 46.25\nCCEvallr 25.29 63.45 33.77 61.48\n13",
    "metadata": {
      "source": "sample.pdf",
      "section": "methods",
      "chunk_id": 48
    }
  },
  {
    "content": "future work may involve designing new strategies,\nsuch as joint training or more efficient interfacing\nmechanisms for both code retrieval and code LLM.\nAcknowledgments\nThis work was supported by National Natural\nScience Foundation of China (No. 62572410,\n42171456), Natural Science Foundation of Xia-\nmen, China (No. 3502Z202471028) and Ant Group\nthrough CCF-Ant Research Fund.",
    "metadata": {
      "source": "sample.pdf",
      "section": "future work",
      "chunk_id": 49
    }
  },
  {
    "content": "References\nLoubna Ben Allal, Raymond Li, Denis Kocetkov,\nChenghao Mou, Christopher Akiki, Carlos Mu\u00f1oz\nFerrandis, Niklas Muennighoff, Mayank Mishra,\nAlex Gu, Manan Dey, Logesh Kumar Umapathi,\nCarolyn Jane Anderson, Yangtian Zi, Joel Lamy-\nPoirier, Hailey Schoelkopf, Sergey Troshin, Dmitry\nAbulkhanov, Manuel Romero, Michael Lappert, and\n22 others. 2023. Santacoder: don\u2019t reach for the\nstars!arXiv Preprint.\nWei Cheng, Yuhan Wu, and Wei Hu. 2024. Dataflow-\nguided retrieval augmentation for repository-level\ncode completion. InACL, pages 7957\u20137977.\nYangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Han-\ntian Ding, Ming Tan, Nihal Jain, Murali Krishna Ra-\nmanathan, Ramesh Nallapati, Parminder Bhatia, Dan\nRoth, and Bing Xiang. 2023. Crosscodeeval: A di-",
    "metadata": {
      "source": "sample.pdf",
      "section": "references",
      "chunk_id": 50
    }
  },
  {
    "content": "manathan, Ramesh Nallapati, Parminder Bhatia, Dan\nRoth, and Bing Xiang. 2023. Crosscodeeval: A di-\nverse and multilingual benchmark for cross-file code\ncompletion. InNeurIPS, pages 46701\u201346723.\nYangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Mu-\nrali Krishna Ramanathan, Ramesh Nallapati, Parmin-\nder Bhatia, Dan Roth, and Bing Xiang. 2024. Co-\ncomic: Code completion by jointly modeling in-file\nand cross-file context. InLREC/COLING, pages\n3433\u20133445.\nGuanting Dong, Yutao Zhu, Chenghao Zhang, Zechen\nWang, Zhicheng Dou, and Ji-Rong Wen. 2024. Un-\nderstand what LLM needs: Dual preference align-\nment for retrieval-augmented generation.arXiv\nPreprint.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo,\nMeng Wang, and Haofen Wang. 2023. Retrieval-",
    "metadata": {
      "source": "sample.pdf",
      "section": "references",
      "chunk_id": 51
    }
  },
  {
    "content": "Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo,\nMeng Wang, and Haofen Wang. 2023. Retrieval-\naugmented generation for large language models: A\nsurvey.arXiv Preprint.\nDaya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming\nZhou, and Jian Yin. 2022. Unixcoder: Unified cross-\nmodal pre-training for code representation. InACL,\npages 7212\u20137225.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. Lora: Low-rank adaptation of\nlarge language models. InICLR.\nBinyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Day-\niheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang,\nBowen Yu, Kai Dang, An Yang, Rui Men, Fei Huang,\nXingzhang Ren, Xuancheng Ren, Jingren Zhou, and\nJunyang Lin. 2024. Qwen2.5-coder technical report.\narXiv Preprint.",
    "metadata": {
      "source": "sample.pdf",
      "section": "references",
      "chunk_id": 52
    }
  },
  {
    "content": "Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and\nJunyang Lin. 2024. Qwen2.5-coder technical report.\narXiv Preprint.\nJiajie Jin, Yutao Zhu, Yujia Zhou, and Zhicheng Dou.\n2024. BIDER: bridging knowledge inconsistency for\nefficient retrieval-augmented llms via key supporting\nevidence. InACL (Findings), pages 750\u2013761.\nKaren Sp\u00e4rck Jones. 2004. A statistical interpretation\nof term specificity and its application in retrieval.J.\nDocumentation, 60(5):493\u2013502.\nJian Li, Yue Wang, Michael R. Lyu, and Irwin King.\n2018. Code completion with neural attention and\npointer networks. InIJCAI, pages 4159\u20134165.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas\nMuennighoff, Denis Kocetkov, Chenghao Mou, Marc\nMarone, Christopher Akiki, Jia Li, Jenny Chim,\nQian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo,",
    "metadata": {
      "source": "sample.pdf",
      "section": "references",
      "chunk_id": 53
    }
  },
  {
    "content": "Marone, Christopher Akiki, Jia Li, Jenny Chim,\nQian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo,\nThomas Wang, Olivier Dehaene, Mishig Davaadorj,\nJoel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko,\nand 48 others. 2023. Starcoder: may the source be\nwith you!Trans. Mach. Learn. Res., 2023.\nMing Liang, Xiaoheng Xie, Gehao Zhang, Xunjin\nZheng, Peng Di, Wei Jiang, Hongwei Chen, Cheng-\npeng Wang, and Gang Fan. 2024. REPOFUSE:\nrepository-level code completion with fused dual con-\ntext.arXiv Preprint.\nWei Liu, Ailun Yu, Daoguang Zan, Bo Shen, Wei Zhang,\nHaiyan Zhao, Zhi Jin, and Qianxiang Wang. 2024.\nGraphcoder: Enhancing repository-level code com-\npletion via code context graph-based retrieval and\nlanguage model.arXiv Preprint.\nAnton Lozhkov, Raymond Li, Loubna Ben Allal, Fed-",
    "metadata": {
      "source": "sample.pdf",
      "section": "references",
      "chunk_id": 54
    }
  },
  {
    "content": "language model.arXiv Preprint.\nAnton Lozhkov, Raymond Li, Loubna Ben Allal, Fed-\nerico Cassano, Joel Lamy-Poirier, Nouamane Tazi,\nAo Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei,\nTianyang Liu, Max Tian, Denis Kocetkov, Arthur\nZucker, Younes Belkada, Zijian Wang, Qian Liu,\nDmitry Abulkhanov, Indraneil Paul, and 38 others.\n2024. Starcoder 2 and the stack v2: The next genera-\ntion.arXiv Preprint.\n11\nYichuan Ma, Yunfan Shao, Peiji Li, Demin Song,\nQipeng Guo, Linyang Li, Xipeng Qiu, and Kai Chen.\n2025. Unitcoder: Scalable iterative code synthesis\nwith unit test guidance.arXiv Preprint.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2023. Codegen: An open large language\nmodel for code with multi-turn program synthesis.\nInICLR.",
    "metadata": {
      "source": "sample.pdf",
      "section": "references",
      "chunk_id": 55
    }
  },
  {
    "content": "Xiong. 2023. Codegen: An open large language\nmodel for code with multi-turn program synthesis.\nInICLR.\nRonak Pradeep, Sahel Sharifymoghaddam, and Jimmy\nLin. 2023. Rankvicuna: Zero-shot listwise document\nreranking with open-source large language models.\narXiv Preprint.\nZhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang,\nJunru Wu, Le Yan, Jiaming Shen, Tianqi Liu, Jialu\nLiu, Donald Metzler, Xuanhui Wang, and Michael\nBendersky. 2024. Large language models are effec-\ntive text rankers with pairwise ranking prompting. In\nNAACL-HLT (Findings), pages 1504\u20131518.\nBaptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom\nKozhevnikov, Ivan Evtimov, Joanna Bitton, Man-",
    "metadata": {
      "source": "sample.pdf",
      "section": "references",
      "chunk_id": 56
    }
  },
  {
    "content": "Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom\nKozhevnikov, Ivan Evtimov, Joanna Bitton, Man-\nish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori,\nWenhan Xiong, Alexandre D\u00e9fossez, Jade Copet, and\n6 others. 2023. Code llama: Open foundation models\nfor code.arXiv Preprint.\nGerard Salton, Anita Wong, and Chung-Shu Yang. 1975.\nA vector space model for automatic indexing.Com-\nmun. ACM, 18(11):613\u2013620.\nParth Sarthi, Salman Abdullah, Aditi Tuli, Shubh\nKhanna, Anna Goldie, and Christopher D. Manning.\n2024. RAPTOR: recursive abstractive processing for\ntree-organized retrieval. InICLR.\nZhihong Shao, Yeyun Gong, Yelong Shen, Minlie\nHuang, Nan Duan, and Weizhu Chen. 2023. Enhanc-\ning retrieval-augmented large language models with\niterative retrieval-generation synergy. InEMNLP",
    "metadata": {
      "source": "sample.pdf",
      "section": "references",
      "chunk_id": 57
    }
  },
  {
    "content": "ing retrieval-augmented large language models with\niterative retrieval-generation synergy. InEMNLP\n(Findings), pages 9248\u20139274.\nDisha Shrivastava, Denis Kocetkov, Harm de Vries,\nDzmitry Bahdanau, and Torsten Scholak. 2023. Re-\npofusion: Training code models to understand your\nrepository.arXiv Preprint.\nWeisong Sun, Chunrong Fang, Yifei Ge, Yuling Hu,\nYuchen Chen, Quanjun Zhang, Xiuting Ge, Yang Liu,\nand Zhenyu Chen. 2024. A survey of source code\nsearch: A 3-dimensional perspective.ACM Trans.\nSoftw. Eng. Methodol., 33(6):166.\nWeiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang\nWang, Pengjie Ren, Zhumin Chen, Dawei Yin, and\nZhaochun Ren. 2023. Is chatgpt good at search?\ninvestigating large language models as re-ranking\nagents. InEMNLP, pages 14918\u201314937.",
    "metadata": {
      "source": "sample.pdf",
      "section": "references",
      "chunk_id": 58
    }
  },
  {
    "content": "investigating large language models as re-ranking\nagents. InEMNLP, pages 14918\u201314937.\nAlexey Svyatkovskiy, Ying Zhao, Shengyu Fu, and Neel\nSundaresan. 2019. Pythia: Ai-assisted code comple-\ntion system. InKDD, pages 2727\u20132735.\nHanzhuo Tan, Qi Luo, Ling Jiang, Zizheng Zhan, Jing\nLi, Haotian Zhang, and Yuqun Zhang. 2024. Prompt-\nbased code completion via multi-retrieval augmented\ngeneration.arXiv Preprint.\nYanlin Wang and Hui Li. 2021. Code completion by\nmodeling flattened abstract syntax trees as graphs. In\nAAAI, pages 14015\u201314023.\nDi Wu, Wasi Uddin Ahmad, Dejiao Zhang, Murali Kr-\nishna Ramanathan, and Xiaofei Ma. 2024. Repo-\nformer: Selective retrieval for repository-level code\ncompletion. InICML, pages 53270\u201353290.\nFengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin",
    "metadata": {
      "source": "sample.pdf",
      "section": "references",
      "chunk_id": 59
    }
  },
  {
    "content": "completion. InICML, pages 53270\u201353290.\nFengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin\nLiu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and\nWeizhu Chen. 2023. Repocoder: Repository-level\ncode completion through iterative retrieval and gen-\neration. InEMNLP, pages 2471\u20132484.\nLingxi Zhang, Yue Yu, Kuan Wang, and Chao Zhang.\n2024. ARL2: aligning retrievers with black-box large\nlanguage models via self-guided adaptive relevance\nlabeling. InACL, pages 3708\u20133719.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, and\n3 others. 2023. A survey of large language models.\narXiv Preprint.",
    "metadata": {
      "source": "sample.pdf",
      "section": "references",
      "chunk_id": 60
    }
  },
  {
    "content": "Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, and\n3 others. 2023. A survey of large language models.\narXiv Preprint.\nShengyao Zhuang, Honglei Zhuang, Bevan Koopman,\nand Guido Zuccon. 2024. A setwise approach for\neffective and highly efficient zero-shot ranking with\nlarge language models. InSIGIR, pages 38\u201347.\nA Results on CCEval\nIn addition to ReccEval, we also conduct experi-\nments on the CCEval benchmark. Tab. 8 reports\nthe results on CCEval using LLM reranker in\nCodeRAG and 100% data for evaluation. Tab. 9\nprovides the results on CCEval using distilled\nreranker in CodeRAG and 30% data for evaluation.\nTab. 10 shows the results using Repoformer-3B\nwith RG-1 as the retriever on CCEval.\nFrom Tab. 8 and Tab. 9, we can see similar trends\nas observed on ReccEval. From Tab. 10, we can",
    "metadata": {
      "source": "sample.pdf",
      "section": "references",
      "chunk_id": 61
    }
  },
  {
    "content": "From Tab. 8 and Tab. 9, we can see similar trends\nas observed on ReccEval. From Tab. 10, we can\nobserve that the performance of Repoformer-3B\nusing both left and right contexts gets significantly\nimproved compared to Repoformer-3B using only\nleft context since it is optimized to use left and\nright parts of the cursor position. By comparing\nCodeRAGllmr with StarCoder2-3B in Tab. 8 and\nRepoformer-3B in Tab. 10, we can observe that\nCodeRAGllmr which only considers left context\noutperforms Repoformer-3B that uses both left and\nright contexts, when 3B code LLM is used as code\ngenerator, showing the effectiveness ofCodeRAG.\n12\nTable 8: Performance on CCEval (Use 100% data for evaluation). Bold and underlined values indicate the best and\nthe second-best results, respectively.",
    "metadata": {
      "source": "sample.pdf",
      "section": "references",
      "chunk_id": 62
    }
  },
  {
    "content": "Gradient Descent minimizes loss by updating parameters.",
    "metadata": {
      "source": "lecture1.pptx",
      "slide_number": 3,
      "document_type": "slides"
    }
  },
  {
    "content": "Backpropagation computes gradients efficiently.",
    "metadata": {
      "source": "lecture1.pptx",
      "slide_number": 4,
      "document_type": "slides"
    }
  },
  {
    "content": "Machine Learning",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 1,
      "document_type": "slides"
    }
  },
  {
    "content": "Arthur Samuel, a pioneer in the field of artificial intelligence and computer gaming, coined the term\u00a0\u201cMachine Learning\u201d as \u2013\u00a0\u201cField of study that gives computers the capability to learn without being explicitly programmed\u201d.\nMachine Learning\nHow it is different from traditional Programming:\nIn Traditional Programming, we feed the Input, Program logic and run the program to get output.\nIn Machine Learning, we feed the input, output and run it on machine during training and the machine creates its own logic, which is being evaluated while testing.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 2,
      "document_type": "slides"
    }
  },
  {
    "content": "Terminologies that one should know before starting Machine Learning:\nModel: A model is a\u00a0specific representation\u00a0learned from data by applying some machine learning algorithm. A model is also called\u00a0hypothesis.\n\nFeature: A feature is an individual measurable property of our data. A set of numeric features can be conveniently described by a\u00a0feature vector. Feature vectors are fed as input to the model. For example, in order to predict a fruit, there may be features like color, smell, taste,\u00a0etc. \n\nTarget(Label): A target variable or label is the value to be predicted by our model. For the fruit example discussed in the features section, the label with each set of input would be the name of the fruit like apple, orange, banana, etc.\nTraining: The idea is to give a set of inputs(features) and it\u2019s expected outputs(labels), so after training, we will have a model (hypothesis) that will then map new data to one of the categories trained on.\n\nPrediction: Once our model is ready, it can be fed a set of inputs to which it will provide a predicted output(label).",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 3,
      "document_type": "slides"
    }
  },
  {
    "content": "Types of Learning\nSupervised Learning\nUnsupervised Learning \nSemi-Supervised Learning\nSupervised Learning: Supervised learning is when the model is getting trained on a labelled dataset.\u00a0Labelled\u00a0dataset is one which have both input and output parameters. In this type of learning both training and validation datasets are labelled as shown in the figures below.\nClassification\nRegression",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 4,
      "document_type": "slides"
    }
  },
  {
    "content": "Types of Supervised Learning:\nClassification \nRegression\nClassification : It is a Supervised Learning task where output is having defined labels(discrete value). For example in above Figure A, Output \u2013 Purchased has defined labels i.e. 0 or 1 ; 1 means the customer will purchase and 0 means that customer won\u2019t purchase. It can be either binary or multi class classification. In\u00a0binary\u00a0classification, model predicts either 0 or 1 ; yes or no but in case of\u00a0multi class classification, model predicts more than one class.\nExample:\u00a0Gmail classifies mails in more than one classes like social, promotions, updates, offers.\nRegression :\u00a0It is a Supervised Learning task where output is having continuous value.\u000bExample in before regression Figure, Output \u2013 Wind Speed is not having any discrete value but is continuous in the particular range. The goal here is to predict a value as much closer to actual output value as our model can and then evaluation is done by calculating error value. The smaller the error the greater the accuracy of our regression model.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 5,
      "document_type": "slides"
    }
  },
  {
    "content": "Linear Regression\n\nNearest Neighbor\n\nGaussian Naive Bayes\n\nDecision Trees\n\nSupport Vector Machine (SVM)\n\nRandom Forest\nExample of Supervised Learning Algorithms:",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 6,
      "document_type": "slides"
    }
  },
  {
    "content": "Unsupervised Learning:\nUnsupervised learning is the training of machine using information that is neither classified nor labeled and allowing the algorithm to act on that information without guidance. Here the task of machine is to group unsorted information according to similarities, patterns and differences without any prior training of data. Unsupervised machine learning is more challenging than supervised learning due to the absence of labels.\nTypes of Unsupervised Learning:\n\nClustering \n\nAssociation",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 7,
      "document_type": "slides"
    }
  },
  {
    "content": "Clustering: A clustering problem is where you want to discover the inherent groupings in the data, such as grouping customers by purchasing behavior.\n\n\nAssociation: \u00a0An association rule learning problem is where you want to discover rules that describe large portions of your data, such as people that buy X also tend to buy Y.\nExamples of unsupervised learning algorithms are:\n\nk-means for clustering problems.\nApriori algorithm for association rule learning problems\nThe most basic disadvantage of any\u00a0Supervised Learning\u00a0algorithm is that the dataset has to be hand-labeled either by a Machine Learning Engineer or a Data Scientist. This is a very\u00a0costly process, especially when dealing with large volumes of data. The most basic disadvantage of any\u00a0Unsupervised Learning\u00a0is that it\u2019s\u00a0application spectrum is limited.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 8,
      "document_type": "slides"
    }
  },
  {
    "content": "Semi-supervised machine learning:\nTo counter these disadvantages, the concept of\u00a0Semi-Supervised Learning\u00a0was introduced. In this type of learning, the algorithm is trained upon a combination of labeled and unlabeled data. Typically, this combination will contain a very small amount of labeled data and a very large amount of unlabeled data.\nIn semi supervised learning labelled data is used to learn a model and using that model unlabeled data is labelled called pseudo labelling now using whole data model is trained for further use",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 9,
      "document_type": "slides"
    }
  },
  {
    "content": "Intuitively, one may imagine the three types of learning algorithms as Supervised learning where a student is under the supervision of a teacher at both home and school, Unsupervised learning where a student has to figure out a concept himself and Semi-Supervised learning where a teacher teaches a few concepts in class and gives questions as homework which are based on similar concepts.\nModel with labellled data and model with both labelled and unlabelled data",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 10,
      "document_type": "slides"
    }
  },
  {
    "content": "REGRESSION\nRegression is a statistical measurement used in finance, investing, and other disciplines that attempts to determine the strength of the relationship between one dependent variable and a series of other changing variables or independent variable",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 11,
      "document_type": "slides"
    }
  },
  {
    "content": "Types of regression\nlinear regression\nSimple linear regression\nMultiple linear regression\nPolynomial regression\nDecision tree regression\nRandom forest regression",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 12,
      "document_type": "slides"
    }
  },
  {
    "content": "Simple Linear regression\nThe simple linear regression models are used to show or predict the relationship between the two variables or factors\nThe factor that being predicted is called dependent variable and the factors that is are used to predict the dependent variable are called independent variables",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 13,
      "document_type": "slides"
    }
  },
  {
    "content": "Predicting C02 emission with engine size feature using simple linear regression",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 14,
      "document_type": "slides"
    }
  },
  {
    "content": "from sklearn import linear_model\n\nregr = linear_model.LinearRegression()\n\ntrain_x = np.asanyarray(train[['ENGINESIZE']])\n\ntrain_y = np.asanyarray(train[['CO2EMISSIONS']])\n\nregr.fit (train_x, train_y)\n\n# The coefficients\n\nprint ('Coefficients: ', regr.coef_)\nprint ('Intercept: ',regr.intercept_)",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 15,
      "document_type": "slides"
    }
  },
  {
    "content": "Multiple linear regression\nMultiple regression\u00a0is an extension of simple\u00a0linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable).",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 16,
      "document_type": "slides"
    }
  },
  {
    "content": "Simple linear regression\nPredict CO2 emission vs Engine size of all cars\n\t- Independent variable(x) : Engine size\n\t-Dependent variable(y):CO2 emission\nMultiple linear regression\nPredict CO2 emission vs Engine size and cylinders of all car\n \t-Independent variable(x) : engine size,cylinders\n       -Dependent variable(y):CO2 emission",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 17,
      "document_type": "slides"
    }
  },
  {
    "content": "from sklearn import linear_model\n\nregr = linear_model.LinearRegression()\n\ntrain_x = np.asanyarray(train[['ENGINESIZE','CYLINDERS']])\n\ntrain_y = np.asanyarray(train[['CO2EMISSIONS']])\n\nregr.fit (train_x, train_y)\n\n# The coefficients\n\nprint ('Coefficients: ', regr.coef_)\nprint ('Intercept: ',regr.intercept_)",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 18,
      "document_type": "slides"
    }
  },
  {
    "content": "Polynomial regression\nPolynomial Regression\u00a0is a form of linear regression in which the relationship between the independent variable x and dependent variable y is modelled as an\u00a0nth\u00a0degree polynomial. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y |x)",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 19,
      "document_type": "slides"
    }
  },
  {
    "content": "from sklearn.preprocessing import PolynomialFeatures\n\nfrom sklearn import linear_model\n\ntrain_x = np.asanyarray(train[['ENGINESIZE','CYLINDERS']])\ntrain_y = np.asanyarray(train[['CO2EMISSIONS']])\n\n\ntest_x = np.asanyarray(test[['ENGINESIZE']])\ntest_y = np.asanyarray(test[['CO2EMISSIONS']])\n\n\npoly = PolynomialFeatures(degree=2)\ntrain_x_poly = poly.fit_transform(train_x)\ntrain_x_poly.shape",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 21,
      "document_type": "slides"
    }
  },
  {
    "content": "fit_transform\u00a0takes our x values, and output a list of our data raised from power of 0 to power of 2 (since we set the degree of our polynomial to 2).\nin our example\nNow, we can deal with it as 'linear regression' problem. Therefore, this polynomial regression is considered to be a special case of traditional multiple linear regression. So, you can use the same mechanism as linear regression to solve such a problems.\nso we can use\u00a0LinearRegression()\u00a0function to solve it:\nclf = linear_model.LinearRegression()\ntrain_y_ = clf.fit(train_x_poly, train_y)\n\n# The coefficients\nprint ('Coefficients: ', clf.coef_)\nprint ('Intercept: ',clf.intercept_)",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 22,
      "document_type": "slides"
    }
  },
  {
    "content": "Decision tree regression\nDecision tree builds regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with\u00a0decision nodes\u00a0and\u00a0leaf nodes. A decision node (e.g., Outlook) has two or more branches (e.g., Sunny, Overcast and Rainy), each representing values for the attribute tested. Leaf node (e.g., Hours Played) represents a decision on the numerical target. The topmost decision node in a tree which corresponds to the best predictor called\u00a0root node. Decision trees can handle both categorical and numerical data.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 23,
      "document_type": "slides"
    }
  },
  {
    "content": "Decision tree regression observes features of an object and trains a model in the structure of a tree to predict data in the future to produce meaningful continuous output. Continuous output means that the output/result is not discrete, i.e., it is not represented just by a discrete, known set of numbers or values. \nDiscrete output example:\u00a0A weather prediction model that predicts whether or not there\u2019ll be rain in a particular day.\u000bContinuous output example:\u00a0A profit prediction model that states the probable profit that can be generated from the sale of a product.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 24,
      "document_type": "slides"
    }
  },
  {
    "content": "Code:\n# import the regressor \nfrom sklearn.tree import DecisionTreeRegressor \n\n# create a regressor object \nregressor = DecisionTreeRegressor(random_state = 0) \n\n# fit the regressor with X and Y data \nregressor.fit(X, y)",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 25,
      "document_type": "slides"
    }
  },
  {
    "content": "Random forest regression\nThe Random Forest is one of the most effective machine learning models for predictive analytics, making it an industrial workhorse for machine learning.\nThe\u00a0random forest\u00a0model is a type of additive model that makes predictions by combining decisions from a sequence of base models. Here, each base classifier is a simple\u00a0decision tree. This broad technique of using multiple models to obtain better predictive performance is called\u00a0model ensembling. In random forests, all the base models are constructed independently using a different subsample\u00a0of the data",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 26,
      "document_type": "slides"
    }
  },
  {
    "content": "Approach :\nPick at random K data points from the training set.\nBuild the decision tree associated with those K data points.\nChoose the number Ntree of trees you want to build and repeat step 1 & 2.\nFor a new data point, make each one of your Ntree trees predict the value of Y for the data point, and assign the new data point the average across all of the predicted Y values.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 27,
      "document_type": "slides"
    }
  },
  {
    "content": "Code\n# import the regressor \nfrom sklearn.tree import DecisionTreeRegressor \n\n# create a regressor object \nregressor = DecisionTreeRegressor(random_state = 0) \n\n# fit the regressor with X and Y data \nregressor.fit(X, y)",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 28,
      "document_type": "slides"
    }
  },
  {
    "content": "Pros and cons",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 29,
      "document_type": "slides"
    }
  },
  {
    "content": "LOGISTIC\nREGRESSION\nIn statistics, the logistic model is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc\n\nBased on the number of categories, Logistic regression can be classified as:\n\nbinomial:\u00a0Target variable can have only 2 possible types: \u201c0\u201d or \u201c1\u201d which may represent \u201cwin\u201d vs \u201closs\u201d, \u201cpass\u201d vs \u201cfail\u201d, \u201cdead\u201d vs \u201calive\u201d, etc.\n\nmultinomial:\u00a0Target variable can have 3 or more possible types which are not ordered(i.e. types have no quantitative significance) like \u201cdisease A\u201d vs \u201cdisease B\u201d vs \u201cdisease C\u201d.\nordinal:\u00a0It deals with target variables with ordered categories. For example, a test score can be categorized as:\u201cvery poor\u201d, \u201cpoor\u201d, \u201cgood\u201d, \u201cvery good\u201d. Here, each category can be given a score like 0, 1, 2, 3.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 30,
      "document_type": "slides"
    }
  },
  {
    "content": "Start with\u00a0binary class problems\nHow do we develop a classification algorithm?\nTumour\u00a0size vs malignancy (0 or 1)\nWe\u00a0could\u00a0use linear regression\nThen threshold the classifier output (i.e. anything over some value is yes, else no)\nIn our example below linear regression with thresholding seems to work",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 31,
      "document_type": "slides"
    }
  },
  {
    "content": "We can see above this does a reasonable job of\u00a0stratifying\u00a0the data points into one of two classes\nBut what if we had a single Yes with a very small\u00a0tumour\u00a0\nThis would lead to classifying all the existing yeses as nos\nAnother issues with linear regression\nWe know Y is 0 or 1\nHypothesis can give values large than 1 or less than 0\nSo, logistic regression generates a value where is always either 0 or 1\nLogistic regression is a\u00a0classification algorithm\u00a0- don't be confused\nHypothesis representation\nWhat function is used to represent our hypothesis in classification\nWe want our classifier to output values between 0 and 1\nWhen using linear regression we did \u00a0h\u03b8(x) =\u00a0(\u03b8T\u00a0x)\nFor classification hypothesis representation we do\u00a0h\u03b8(x)\u00a0= g((\u03b8T\u00a0x))\nWhere we define g(z)\nz is a real number\nThis is the\u00a0sigmoid function, or the\u00a0logistic function\nIf we combine these equations we can write out the hypothesis as",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 32,
      "document_type": "slides"
    }
  },
  {
    "content": "How  does the sigmoid function look like\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCrosses 0.5 at the origin, then flattens out]\nAsymptotes\u00a0at 0 and 1",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 33,
      "document_type": "slides"
    }
  },
  {
    "content": "Interpreting\u00a0hypothesis output\n\nWhen our hypothesis (h\u03b8(x))\u00a0outputs a number, we treat that value as the estimated probability that y=1 on input x\nExample\nIf X is a feature vector with\u00a0x0\u00a0= 1 (as always) and\u00a0x1\u00a0=\u00a0tumourSize\nh\u03b8(x) = 0.7\nTells a patient they have a 70% chance of a tumor being malignant\nh\u03b8(x) = P(y=1|x ;\u00a0\u03b8)\nWhat does this mean?\nProbability that y=1, given x, parameterized by\u00a0\u03b8\nSince this is a binary classification task we know y = 0 or 1\nSo the following must be true\nP(y=1|x ;\u00a0\u03b8) +\u00a0P(y=0|x ;\u00a0\u03b8) = 1\nP(y=0|x ;\u00a0\u03b8) = 1 -\u00a0P(y=1|x ;\u00a0\u03b8)",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 34,
      "document_type": "slides"
    }
  },
  {
    "content": "Decision boundary\nThis gives a better sense of what the hypothesis function is computing\nOne way of using the sigmoid function is;\nWhen the probability of y being 1 is greater than 0.5 then we can predict y = 1\nElse we predict y = 0\nWhen is it exactly that\u00a0h\u03b8(x) is greater than 0.5?\nLook at sigmoid function\ng(z) is greater than or equal to 0.5 when z is greater than or equal to 0\nSo\u00a0if z is\u00a0positive, g(z) is greater than 0.5\nz =\u00a0(\u03b8T\u00a0x)\nSo when\u00a0\n\u03b8T\u00a0x >= 0\u00a0\nThen\u00a0h\u03b8\u00a0>= 0.5\n\n\n\nSo what we've shown is that the hypothesis predicts y = 1 when\u00a0\u03b8T\u00a0x >= 0\u00a0\nThe\u00a0corollary\u00a0of that when\u00a0\u03b8T\u00a0x <= 0 then the hypothesis predicts y = 0\nLet's use this to better understand how the hypothesis makes its predictions",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 35,
      "document_type": "slides"
    }
  },
  {
    "content": "Decision boundary\nThis gives a better sense of what the hypothesis function is computing\nOne way of using the sigmoid function is;\nWhen the probability of y being 1 is greater than 0.5 then we can predict y = 1\nElse we predict y = 0\nWhen is it exactly that\u00a0h\u03b8(x) is greater than 0.5?\nLook at sigmoid function\ng(z) is greater than or equal to 0.5 when z is greater than or equal to 0\nSo\u00a0if z is\u00a0positive, g(z) is greater than 0.5\nz =\u00a0(\u03b8T\u00a0x)\nSo when\u00a0\n\u03b8T\u00a0x >= 0\u00a0\nThen\u00a0h\u03b8\u00a0>= 0.5\n\n\n\nSo what we've shown is that the hypothesis predicts y = 1 when\u00a0\u03b8T\u00a0x >= 0\u00a0\nThe\u00a0corollary\u00a0of that when\u00a0\u03b8T\u00a0x <= 0 then the hypothesis predicts y = 0\nLet's use this to better understand how the hypothesis makes its predictions",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 36,
      "document_type": "slides"
    }
  },
  {
    "content": "Consider,\n   h\u03b8(x) =\u00a0g(\u03b80\u00a0+\u00a0\u03b81x1\u00a0+\u00a0\u03b82x2)\nSo, for example\n\u03b80\u00a0= -3\n\u03b81\u00a0= 1\n\u03b82\u00a0= 1\nSo our parameter vector is a column vector with the above values\nSo,\u00a0\u03b8T\u00a0is a row vector = [-3,1,1]\nWhat does this mean?\nThe z here becomes\u00a0\u03b8T\u00a0x\nWe predict \"y = 1\" if\n-3x0\u00a0+ 1x1\u00a0+ 1x2\u00a0>= 0\n-3\u00a0+ x1\u00a0+ x2\u00a0>= 0\nWe can also re-write this as\nIf (x1\u00a0+ x2\u00a0>= 3) then we predict y = 1\nIf we plot\nx1\u00a0+ x2\u00a0= 3 we graphically plot our\u00a0decision boundary",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 37,
      "document_type": "slides"
    }
  },
  {
    "content": "h\u03b8(x) =\u00a0g(\u03b80\u00a0+\u00a0\u03b81x1+\u00a0\u03b83x12\u00a0+\u00a0\u03b84x22)\nSay\u00a0\u03b8T\u00a0was [-1,0,0,1,1] then we say;\nPredict that \"y = 1\"\u00a0if\n-1 + x12\u00a0+\u00a0x22\u00a0>= 0\u000bor\nx12\u00a0+\u00a0x22\u00a0>= 1\nIf we plot\u00a0x12\u00a0+\u00a0x22\u00a0= 1",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 38,
      "document_type": "slides"
    }
  },
  {
    "content": "Cost function for logistic regression\nLinear\u00a0regression\u00a0uses the following function to\u00a0determine\u00a0\u03b8",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 39,
      "document_type": "slides"
    }
  },
  {
    "content": "If we use this function for logistic regression\u00a0this is a\u00a0non-convex function\u00a0for parameter optimization    Could work !!!\nWhat do we mean by non convex?\nWe have some function - J(\u03b8) - for determining the parameters\nOur hypothesis function has a non-linearity (sigmoid\u00a0function of\u00a0h\u03b8(x) )\nThis is a complicated non-linear function\nIf you take\u00a0h\u03b8(x)\u00a0and plug it into the Cost() function, and them plug the Cost() function into J(\u03b8) and plot\u00a0J(\u03b8) we find many local optimum ->\u00a0non convex function\nWhy is this a problem\nLots of local\u00a0minima\u00a0mean gradient descent may not find the\u00a0global\u00a0optimum - may get stuck in a global minimum\nWe would like a convex function so if you run gradient descent you converge to a global minimum",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 40,
      "document_type": "slides"
    }
  },
  {
    "content": "A convex logistic regression cost function\n\nTo get around this we need a different, convex Cost() function which means we can apply gradient descent\nThe above two functions can be compressed into a single function i.e.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 41,
      "document_type": "slides"
    }
  },
  {
    "content": "Gradient Descent\n\n\nNow the question arises, how do we reduce the cost value. Well, this can be done by using\u00a0Gradient Descent.\u00a0The main goal of Gradient descent is to\u00a0minimize the cost value.\u00a0i.e. min J(\u03b8).\nNow to minimize our cost function we need to run the gradient descent function on each parameter i.e.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 42,
      "document_type": "slides"
    }
  },
  {
    "content": "Gradient descent has an analogy in which we have to imagine ourselves at the top of a mountain valley and left stranded and blindfolded, our objective is to reach the bottom of the hill. Feeling the slope of the terrain around you is what everyone would do. Well, this action is analogous to calculating the gradient descent, and taking a step is analogous to one iteration of the update to the parameters.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 43,
      "document_type": "slides"
    }
  },
  {
    "content": "Multiclass\u00a0classification problems\n\nGetting logistic regression for multiclass classification using\u00a0one vs. all\nMulticlass - more than yes or no (1 or 0)\nClassification with multiple classes for assignment",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 44,
      "document_type": "slides"
    }
  },
  {
    "content": "Given a dataset with three classes, how do we get a learning algorithm to work?\nUse one vs. all classification make binary classification work for multiclass classification\nOne vs. all classification\nSplit the training set into three\u00a0separate\u00a0binary classification problems\ni.e. create a new fake training set\nTriangle (1) vs crosses and squares (0)\u00a0h\u03b81(x)\nP(y=1 | x1;\u00a0\u03b8)\nCrosses (1) vs triangle and square (0)\u00a0h\u03b82(x)\nP(y=1 |\u00a0x2;\u00a0\u03b8)\nSquare (1) vs crosses and square (0)\u00a0h\u03b83(x)\nP(y=1 |\u00a0x3;\u00a0\u03b8)\nTrain a logistic regression classifier\u00a0h\u03b8(i)(x) for each class i to predict the probability that y = i\nOn a new input,\u00a0x\u00a0to make a prediction, pick the class\u00a0i\u00a0that maximizes the probability that\u00a0h\u03b8(i)(x) = 1",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 45,
      "document_type": "slides"
    }
  },
  {
    "content": "K-Nearest Neighbors",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 46,
      "document_type": "slides"
    }
  },
  {
    "content": "This algorithm classifies cases based on their similarity to other cases.\u00a0\n\nIn K-Nearest Neighbors, data points that are near each other are said to be neighbors.\u00a0\n\nK-Nearest Neighbors is based on this paradigm.\u00a0\n\nSimilar cases with the same class labels are near each other.\u00a0\nThus, the distance between two cases is a measure of their dissimilarity.\u00a0\nThere are different ways to calculate the similarity or conversely,\u00a0\nthe distance or dissimilarity of two data points.\u00a0\nFor example, this can be done using Euclidean distance.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 47,
      "document_type": "slides"
    }
  },
  {
    "content": "the K-Nearest Neighbors algorithm works as follows.\u00a0\n\npick a value for K. \n\u00a0\ncalculate the distance from the new case hold out from each of the cases in the dataset.\u00a0\n\nsearch for the K-observations in\u00a0the training data that are nearest to the measurements of the unknown data point.\u00a0\n\npredict the response of the unknown data point\u00a0using the most popular response value from the K-Nearest Neighbors.\u00a0\n\n\nThere are two parts in this algorithm that might be a bit confusing. \n\nFirst, how to select the correct K \n\nsecond,\u00a0how to compute the similarity between cases,\u00a0\n\n\t\tLet's first start with the second concern.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 48,
      "document_type": "slides"
    }
  },
  {
    "content": "How to select the correct K\nAs mentioned, K and K-Nearest Neighbors is the number of nearest neighbors to examine.\u00a0\n\nIt is supposed to be specified by the user.\u00a0\nSo, how do we choose the right K?\u00a0\n\nAssume that we want to find the class of\u00a0\nthe customer noted as question mark on the chart.\u00a0\n\nWhat happens if we choose a very low value of K?\u00a0\nLet's say, K equals one.\u00a0\nThe first nearest point would be blue,\u00a0\nwhich is class one.\u00a0\nThis would be a bad prediction,\u00a0\nsince more of the points around it are magenta or class four.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 50,
      "document_type": "slides"
    }
  },
  {
    "content": "In fact, since its nearest neighbor is blue we can say that we capture\u00a0the noise in the data or we chose one of the points that was an anomaly in the data.\u00a0\n\nA low value of K causes a highly complex model as well,\u00a0which might result in overfitting of the model.\u00a0\n\nIt means the prediction process is not\u00a0generalized enough to be used for out-of-sample cases.\u00a0\n\nOut-of-sample data is data that is outside of the data set used to train the model.\u00a0\n\nIn other words, it cannot be trusted to be used for prediction of unknown samples.\u00a0It's important to remember that overfitting is bad,\u00a0as we want a general model that works for any data,\u00a0not just the data used for training.\u00a0\n\nNow, on the opposite side of the spectrum,\u00a0if we choose a very high value of K such as K equals 20,\u00a0\nthen the model becomes overly generalized.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 51,
      "document_type": "slides"
    }
  },
  {
    "content": "So, how can we find the best value for K?\u00a0\n\nThe general solution is to reserve a part of\u00a0your data for testing the accuracy of the model.\u00a0\nOnce you've done so,\u00a0 choose K equals one and then use the training part for modeling\u00a0 and calculate the accuracy of prediction using all samples in your test set.\u00a0\n\nRepeat this process increasing the K and see which K is best for your model.\u00a0\nFor example, in our case,\u00a0\n\nK equals four will give us the best accuracy.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 52,
      "document_type": "slides"
    }
  },
  {
    "content": "Advantages of KNN\u000b\u000b1. No Training Period:\u00a0KNN is called\u00a0Lazy Learner (Instance based learning). It does not learn anything in the training period.\u00a0It does not derive any discriminative function from the training data.\u00a0In other words, there is no training period for it. It stores the training dataset and learns from it only at the time of making real time predictions. This makes the KNN algorithm much faster than other algorithms that require training e.g. SVM, Linear Regression etc.\u000b\u000b2.\u00a0Since the KNN algorithm requires no training before making predictions,\u00a0new data can be added seamlessly\u00a0which will not impact the accuracy of the algorithm.\u000b\u000b3.\u00a0KNN is very\u00a0easy to implement. There are only two parameters required to implement KNN i.e. the value of K and the distance function (e.g. Euclidean or Manhattan etc.)",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 53,
      "document_type": "slides"
    }
  },
  {
    "content": "Disadvantages of KNN\u000b\u000b1. Does not work well with large dataset:\u00a0In large datasets, the cost of calculating the distance between the new point and each existing points is huge which degrades the performance of the algorithm.\u000b\u000b2. Does not work well with high dimensions:\u00a0The KNN algorithm doesn't work well with high dimensional data because with large number of dimensions, it becomes difficult for the algorithm to calculate the distance in each dimension.\u000b\u000b3.Sensitive to noisy data, missing values and outliers: KNN is sensitive to noise in the dataset. We need to manually impute missing values and remove outliers.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 54,
      "document_type": "slides"
    }
  },
  {
    "content": "SUPPORT VECTOR MACHINE(SVM)\nA Support Vector Machine is a supervised algorithm\u00a0that can classify cases by finding a separator.\n\u00a0\nSVM works by first mapping data to a high dimensional feature space so that data\u00a0points can be categorized, even when the data are not linearly separable.\u00a0\n\nThen, a separator is estimated for the data.\u00a0The data should be transformed in such a way\u00a0that a separator could be drawn as a hyperplane.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 56,
      "document_type": "slides"
    }
  },
  {
    "content": "Therefore, the SVM algorithm\u00a0 outputs an optimal hyperplane that categorizes new examples.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 57,
      "document_type": "slides"
    }
  },
  {
    "content": "DATA TRANFORMATION\nFor the sake of simplicity, imagine that our dataset is one-dimensional data.\nThis means we have only one feature x.\u00a0\nAs you can see, it is not linearly separable.\u00a0\nWell, we can transfer it into a two-dimensional space.\u00a0For example, you can increase the dimension of data by mapping x into\u00a0a new space using a function with outputs x and x squared.\nBasically, mapping data into a higher-dimensional space is called,\u00a0 kernelling.\nThe mathematical function used for the transformation is known as the kernel\u00a0\nfunction, and can be of different types,such as linear,\u00a0polynomial, Radial Basis Function, or RBF, and sigmoid.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 58,
      "document_type": "slides"
    }
  },
  {
    "content": "SVMs are based on the idea of finding a hyperplane\u00a0 that best divides a data set into two classes as shown here. \nAs we're in a two-dimensional space, you can think of the hyperplane\u00a0as a line that linearly separates the blue points from the red points.\nADVANTAGES\n- Accurate in high dimension place\nMemory efficient\n\nDISADVANTAGES\n- Small datasets\n- Prone to overfitting\n\n  APPLICATIONS\nImage Recognition\nSpam detection",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 59,
      "document_type": "slides"
    }
  },
  {
    "content": "Naive Bayes Classifiers\ncollection of classification algorithms",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 60,
      "document_type": "slides"
    }
  },
  {
    "content": "Principle of Naive Bayes Classifier:\nA Naive Bayes classifier is a probabilistic machine learning model that\u2019s used for classification task. The crux of the classifier is based on the Bayes theorem.\n\nBayes theorem can be rewritten as:\n\n\nIt is not a single algorithm but a family of algorithms where all of them share a common principle, i.e. every pair of features being classified is independent of each other.\nBayes theorem can be rewritten as:",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 61,
      "document_type": "slides"
    }
  },
  {
    "content": "Example:\u000bLet us take an example to get some better intuition. Consider the problem of playing golf. The dataset is represented as below.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 62,
      "document_type": "slides"
    }
  },
  {
    "content": "We classify whether the day is suitable for playing golf, given the features of the day. The columns represent these features and the rows represent individual entries. If we take the first row of the dataset, we can observe that is not suitable for playing golf if the outlook is rainy, temperature is hot, humidity is high and it is not windy. We make two assumptions here, one as stated above we consider that these predictors are independent. That is, if the temperature is hot, it does not necessarily mean that the humidity is high.\u00a0Another assumption made here is that all the predictors have an equal effect on the outcome.\u00a0That is, the day being windy does not have more importance in deciding to play golf or not.\nAccording to this example, Bayes theorem can be rewritten as:\n\n\nThe variable\u00a0y\u00a0is the class variable(play golf), which represents if it is suitable to play golf or not given the conditions. Variable\u00a0X\u00a0represent the parameters/features.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 63,
      "document_type": "slides"
    }
  },
  {
    "content": "X\u00a0is given as,\nHere x_1,x_2\u2026.x_n represent the features, i.e they can be mapped to outlook, temperature, humidity and windy. By substituting for\u00a0X\u00a0and expanding using the chain rule we get,\n\nNow, you can obtain the values for each by looking at the dataset and substitute them into the equation. For all entries in the dataset, the denominator does not change, it remain static. Therefore, the denominator can be removed and a proportionality can be introduced.\n\nIn our case, the class variable(y) has only two outcomes, yes or no. There could be cases where the classification could be multivariate. Therefore, we need to find the class\u00a0y\u00a0with maximum probability.\n\nUsing the above function, we can obtain the class, given the predictors.\nIn our case, the class variable(y) has only two outcomes, yes or no. There could be cases where the classification could be multivariate. Therefore, we need to find the class\u00a0y\u00a0with maximum probability.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 64,
      "document_type": "slides"
    }
  },
  {
    "content": "We need to find P(xi\u00a0| yj) for each xi\u00a0in X and yj\u00a0in y. All these calculations have been demonstrated in the tables below:\n\n\n\n\n\n\n\n\n\n\nSo, in the figure above, we have calculated P(xi\u00a0| yj) for each xi\u00a0in X and yj\u00a0in y manually in the tables 1-4. For example, probability of playing golf given that the temperature is cool, i.e P(temp. = cool | play golf = Yes) = 3/9.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 65,
      "document_type": "slides"
    }
  },
  {
    "content": "Also, we need to find class probabilities (P(y)) which has been calculated in the table 5. For example, P(play golf = Yes) = 9/14.\nSo now, we are done with our pre-computations and the classifier is ready!\nLet us test it on a new set of features (let us call it today):",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 66,
      "document_type": "slides"
    }
  },
  {
    "content": "Types of Naive Bayes Classifier:\nMultinomial Naive Bayes: This is mostly used for document classification problem, i.e whether a  document belongs to the category of sports, politics, technology etc. The features/predictors used by the classifier are the frequency of the words present.\n\nBernoulli Naive Bayes: This is similar to the multinomial naive bayes but the predictors are boolean variables. The parameters that we use to predict the class variable take up only values yes or no, for example if a word occurs in the text or not.\n\nGaussian Naive Bayes: When the predictors take up a continuous value and are not discrete, we assume that these values are sampled from a gaussian distribution.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 67,
      "document_type": "slides"
    }
  },
  {
    "content": "Gaussian Distribution(Normal Distribution)\nConclusion:\nNaive Bayes algorithms are mostly used in sentiment analysis, spam filtering, recommendation systems etc. They are fast and easy to implement but their biggest disadvantage is that the requirement of predictors to be independent. In most of the real life cases, the predictors are dependent, this hinders the performance of the classifier.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 68,
      "document_type": "slides"
    }
  },
  {
    "content": "Decision Tree\nClassification Algorithm",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 69,
      "document_type": "slides"
    }
  },
  {
    "content": "Decision tree algorithm falls under the category of supervised learning. They can be used to solve both regression and classification problems..\nDecision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with\u00a0decision nodes\u00a0and\u00a0leaf nodes. A decision node (e.g., Outlook) has two or more branches (e.g., Sunny, Overcast and Rainy). Leaf node (e.g., Play) represents a classification or decision. The topmost decision node in a tree which corresponds to the best predictor called\u00a0root node. Decision trees can handle both categorical and numerical data.\u00a0\nWe can represent any boolean function on discrete attributes using the decision tree.\nTypes of decision trees\nCategorical Variable Decision Tree: Decision Tree which has categorical target variable then it called as categorical variable decision tree.\nContinuous Variable Decision Tree: Decision Tree which has continuous target variable then it is called as Continuous Variable Decision Tree.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 70,
      "document_type": "slides"
    }
  },
  {
    "content": "Root Node:\u00a0It represents entire population or sample and this further gets divided into two or more homogeneous sets.\nSplitting:\u00a0It is a process of dividing a node into two or more sub-nodes.\nDecision Node:\u00a0When a sub-node splits into further sub-nodes, then it is called decision node.\nLeaf/ Terminal Node:\u00a0Nodes with no children (no further split) is called Leaf or Terminal node.\nPruning:\u00a0When we reduce the size of decision\n  trees by removing nodes (opposite of Splitting), \n  the process is called pruning.\nBranch / Sub-Tree:\u00a0A sub section of decision \n tree is called branch or sub-tree.\nParent and Child Node:\u00a0A node, which is divided\n  into sub-nodes is called parent node of sub-nodes\n  where as sub-nodes are the child of parent node.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 71,
      "document_type": "slides"
    }
  },
  {
    "content": "Algorithm\nAlgorithms used in decision trees:\nID3\nGini Index\nChi-Square\nReduction in Variance\nThe core algorithm for building decision trees is called\u00a0ID3.\u00a0Developed\u00a0by J. R. Quinlan and it uses\u00a0Entropy\u00a0and\u00a0Information Gain\u00a0to construct a decision tree.\nThe ID3 algorithm begins with the original set S as the root node. On each iteration of the algorithm, it iterates through every unused attribute of the set S and calculates the entropy  H(S)or information gain IG(S) of that attribute. It then selects the attribute which has the smallest entropy (or largest information gain) value. The set S is then split or partitioned by the selected attribute to produce subsets of the data",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 72,
      "document_type": "slides"
    }
  },
  {
    "content": "Entropy\nEntropy is a measure of the randomness in the information being processed. The higher the entropy, the harder it is to draw any conclusions from that information. Decision tree algorithm uses entropy to calculate the homogeneity of a sample. If the sample is completely homogeneous the entropy is zero and if the sample is an equally divided it has entropy of one.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 73,
      "document_type": "slides"
    }
  },
  {
    "content": "Example:",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 74,
      "document_type": "slides"
    }
  },
  {
    "content": "To build a decision tree, we need to calculate two types of entropy using frequency tables as follows:\na) Entropy using the frequency table of one attribute:",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 75,
      "document_type": "slides"
    }
  },
  {
    "content": "b) Entropy using the frequency table of two attributes:",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 76,
      "document_type": "slides"
    }
  },
  {
    "content": "Information gain",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 77,
      "document_type": "slides"
    }
  },
  {
    "content": "Step 2: The dataset is then split on the different attributes. The entropy for each branch is calculated. Then it is added proportionally, to get total entropy for the split. The resulting entropy is subtracted from the entropy before the split. The result is the Information Gain, or decrease in entropy.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 78,
      "document_type": "slides"
    }
  },
  {
    "content": "Step 3: Choose attribute with the largest information gain as the decision node, divide the dataset by its branches and repeat the same process on every branch.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 79,
      "document_type": "slides"
    }
  },
  {
    "content": "Step 4a: A branch with entropy of 0 is a leaf node\n\n\n\n\n\nStep 4b: A branch with entropy more than 0 needs further splitting",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 80,
      "document_type": "slides"
    }
  },
  {
    "content": "Step 5: The ID3 algorithm is run recursively on the non-leaf branches, until all data is classified.\nDecision Tree to Decision Rules\nA decision tree can easily be transformed to a set of rules by mapping from the root node to the leaf nodes one by one.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 81,
      "document_type": "slides"
    }
  },
  {
    "content": "Limitations to Decision Trees\nDecision trees tend to have high variance when they utilize different training and test sets of the same data, since they tend to overfit on training data. This leads to poor performance on unseen data. Unfortunately, this limits the usage of decision trees in predictive modeling. \nTo overcome these problems we use ensemble methods, we can create models that utilize underlying(weak) decision trees as a foundation for producing powerful results and this is done in Random Forest Algorithm",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 82,
      "document_type": "slides"
    }
  },
  {
    "content": "Random forest",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 83,
      "document_type": "slides"
    }
  },
  {
    "content": "Definition:\nRandom forest algorithm is a supervised classification algorithm Based on Decision Trees, also known as random decision forests, are a popular ensemble method that can be used to build\u00a0predictive models\u00a0for both classification and regression problems.\nEnsemble we mean(In Random Forest Context), Collective Decisions of Different Decision Trees. In RFT(Random Forest Tree), we make a prediction about the class, not simply based on One Decision Trees, but by an (almost) Unanimous Prediction, made by 'K'\u00a0Decision Trees.\nConstruction:\n'K' Individual Decision Trees are made from given Dataset, by randomly dividing the Dataset and the Feature Subspace by process called as\u00a0Bootstrap Aggregation(Bagging), which is process of random selection with replacement. Generally 2/3rd of the Dataset (row-wise 2/3rd) is selected by bagging, and On that Selected Dataset we perform what we call is Attribute Bagging.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 84,
      "document_type": "slides"
    }
  },
  {
    "content": "Now\u00a0Attribute Bagging\u00a0is done to select 'm' features from given M features,(this Process is also called Random Subspace Creation.) Generally value of 'm' is square-root of M. Now we select say, 10 such values of m, and then Build 10 Decision Trees based on them, and test the 1/3rd remaining Dataset on these(10 Decision Trees).We would then Select the Best Decision Tree out of this. And Repeat the whole Process 'K' times again to build such 'K' decision trees.\nClassification:\nPrediction in Random Forest (a collection of 'K' Decision Trees) is truly ensemble ie, For Each Decision Tree, Predict the class of Instance and then return the class which was predicted the most often.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 85,
      "document_type": "slides"
    }
  },
  {
    "content": "Using Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier //Importing library\n TRAIN_DIR = \"../train-mails\"\n TEST_DIR = \"../test-mails\"\n dictionary = make_Dictionary(TRAIN_DIR)\n print \"reading and processing emails from file.\"\n features_matrix, labels = extract_features(TRAIN_DIR)\n test_feature_matrix, test_labels = extract_features(TEST_DIR)\n model = RandomForestClassifier() //Creating model\n print \"Training model.\"\n model.fit(features_matrix, labels) //training model\n predicted_labels = model.predict(test_feature_matrix)\n print \"FINISHED classifying. accuracy score : \"\n print accuracy_score(test_labels, predicted_labels) //Predicting\n we will get accuracy around 95.7%.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 86,
      "document_type": "slides"
    }
  },
  {
    "content": "Parameters\nLets understand and try with some of the tuning parameters.\nn_estimators :\u00a0Number of trees in forest. Default is 10.\ncriterion:\u00a0\u201cgini\u201d or \u201centropy\u201d same as decision tree classifier.\nmin_samples_split:\u00a0minimum number of working set size at node required to split. Default is 2.\nPlay with these parameters by changing values individually and in combination and check if you can improve accuracy.\ntrying following combination and obtained the accuracy as shown in next slide image .",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 87,
      "document_type": "slides"
    }
  },
  {
    "content": "Final Thoughts\nRandom Forest Classifier being ensembled algorithm tends to give more accurate result. This is because it works on principle,\nNumber of weak estimators when combined forms strong estimator.\nEven if one or few decision trees are prone to a noise, overall result would tend to be correct. Even with small number of estimators = 30 it gave us high accuracy as 97%.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 89,
      "document_type": "slides"
    }
  },
  {
    "content": "Clustering\nUnsupervised learning",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 90,
      "document_type": "slides"
    }
  },
  {
    "content": "Clustering\nA cluster is a subset of data which are similar. \nClustering (also called unsupervised learning) is the process of dividing a dataset into groups such that the members of each group are as similar (close) as possible to one another, and different groups are as dissimilar (far) as possible from one another.\nGenerally, it is used as a process to find meaningful structure, generative features, and groupings inherent in a set of examples.\n Clustering can uncover previously undetected relationships in a dataset. There are many applications for cluster analysis. For example, in business, cluster analysis can be used to discover and characterize customer segments for marketing purposes and in biology, it can be used for classification of plants and animals given their features.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 91,
      "document_type": "slides"
    }
  },
  {
    "content": "Clustering Algorithms\nK-means Algorithm\nThe simplest among unsupervised learning algorithms. This works on the principle of k-means clustering. This actually means that the clustered groups (clusters) for a given set of data are represented by a variable \u2018k\u2019. For each cluster, a centroid (arithmetic mean of all the data points that belong to that cluster) is defined. \nThe centroid is a data point present at the centre of each cluster (considering Euclidean distance). The trick is to define the centroids far away from each other so that the variation is less. After this, each data point in the cluster is assigned to the nearest centroid  such that the sum of the squared distance between the data points and the cluster\u2019s centroid is at the minimum.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 92,
      "document_type": "slides"
    }
  },
  {
    "content": "Algorithm\t\t\n1.Clusters the data into k groups where k  is predefined.\n2. k points at random as cluster centers.\n3.Assign objects to their closest cluster center according to the Euclidean distance function.\n4.Calculate the centroid or mean of all objects in each cluster.\n5.Repeat steps 2, 3 and 4 until the same points are assigned to each cluster in consecutive rounds.\nThe Euclidean distance between two points in either the plane or 3-dimensional space measures the length of a segment connecting the two points.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 93,
      "document_type": "slides"
    }
  },
  {
    "content": "The step by step process:",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 94,
      "document_type": "slides"
    }
  },
  {
    "content": "K-means clustering algorithm has found to be very useful in grouping new data. Some practical applications which use k-means clustering are sensor measurements, activity monitoring in a manufacturing process, audio detection and image segmentation.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 95,
      "document_type": "slides"
    }
  },
  {
    "content": "Disadvantage Of K-MEANS:\nK-Means forms spherical clusters only. This algorithm fails when data is not spherical ( i.e. same variance in all directions).\nK-Means algorithm is sensitive towards outlier. Outliers can skew the clusters in K-Means in very large extent.\nK-Means algorithm requires one to specify the number of clusters and for which there is no global method to choose best value.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 96,
      "document_type": "slides"
    }
  },
  {
    "content": "Hierarchical Clustering Algorithms\nLast but not the least are the hierarchical clustering algorithms. These algorithms have clusters sorted in an order based on the hierarchy in data similarity observations.\u00a0Hierarchical clustering\u00a0is categorised into two types, divisive(top-down) clustering and agglomerative (bottom-up) clustering. \nAgglomerative Hierarchical clustering Technique:\u00a0In this technique, initially each data point is considered as an individual cluster. At each iteration, the similar clusters merge with other clusters until one cluster or K clusters are formed.\nDivisive\u00a0Hierarchical clustering Technique:Divisive Hierarchical clustering is exactly the opposite of the\u00a0Agglomerative Hierarchical clustering.\u00a0In Divisive Hierarchical clustering, we consider all the data points as a single cluster and in each iteration, we separate the data points from the cluster which are not similar. Each data point which is separated is considered as an individual cluster.\nMost of the hierarchical algorithms such as\u00a0single linkage, complete linkage,\u00a0median linkage, Ward\u2019s method, among others, follow the agglomerative approach.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 97,
      "document_type": "slides"
    }
  },
  {
    "content": "Calculating the similarity between two clusters is important to merge or divide the clusters. There are certain approaches which are used to calculate the similarity between two clusters:\nMIN:\u00a0Also known as single linkage algorithm can be defined as\u00a0the similarity of two clusters C1 and C2 is equal to the\u00a0minimum\u00a0of the similarity between points Pi and Pj such that Pi belongs to C1 and Pj belongs to C2.\nThis approach can separate non-elliptical shapes as long as the gap between two clusters is not small.\nMIN approach cannot separate clusters properly if there is noise between clusters.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 99,
      "document_type": "slides"
    }
  },
  {
    "content": "MAX:\u00a0Also known as the complete linkage algorithm, this is exactly opposite to the\u00a0MIN\u00a0approach. The similarity of two clusters C1 and C2 is equal to the\u00a0maximum\u00a0of the similarity between points Pi and Pj such that Pi belongs to C1 and Pj belongs to C2.\nMAX approach does well in separating clusters if there is noise between clusters but Max approach tends to break large clusters.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 100,
      "document_type": "slides"
    }
  },
  {
    "content": "Group Average:\u00a0Take all the pairs of points and compute their similarities and calculate the average of the similarities.\nThe group Average approach does well in separating clusters if there is noise between clusters but it is less popular technique in the real world.\nLimitations of\u00a0Hierarchical clustering Technique:\nThere is no mathematical objective for Hierarchical clustering.\nAll the approaches to calculate the similarity between clusters has its own disadvantages.\nHigh space and time complexity for Hierarchical clustering. Hence this clustering algorithm cannot be used when we have huge data.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 101,
      "document_type": "slides"
    }
  },
  {
    "content": "Density-based spatial clustering of applications with noise (DBSCAN)\u00a0is a well-known data clustering algorithm that is commonly used in data mining and machine learning.\nUnlike to\u00a0K-means, DBSCAN does not require the user to specify the number of clusters to be generated\nDBSCAN can find any shape of clusters. The cluster doesn\u2019t have to be circular.\nDBSCAN can identify outliers\nThe basic idea behind\u00a0density-based clustering\u00a0approach is derived from a human intuitive clustering method. by looking at the figure below, one can easily identify four clusters along with several points of noise, because of the differences in the density of points",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 102,
      "document_type": "slides"
    }
  },
  {
    "content": "DBSCAN algorithm has two parameters:\n\u025b: The radius of our neighborhoods around a data point\u00a0p.\nminPts: The minimum number of data points we want in a neighborhood to define a cluster.\nUsing these two parameters, DBSCAN categories the data points into three categories:\nCore Points: A data point\u00a0p\u00a0is a\u00a0core point\u00a0if\u00a0Nbhd(p,\u025b) [\u025b-neighborhood of\u00a0p] contains at least\u00a0minPts\u00a0; |Nbhd(p,\u025b)| >=\u00a0minPts.\nBorder Points: A data point *q\u00a0is a\u00a0border point\u00a0if\u00a0Nbhd(q,\u00a0\u025b) contains less than\u00a0minPts\u00a0data points, but\u00a0q\u00a0is\u00a0reachable\u00a0from some\u00a0core point\u00a0p.\nOutlier: A data point\u00a0o\u00a0is an\u00a0outlier\u00a0if it is neither a core point nor a border point. Essentially, this is the \u201cother\u201d class.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 103,
      "document_type": "slides"
    }
  },
  {
    "content": "The steps to the DBSCAN algorithm are:\nPick a point at random that has not been assigned to a cluster or been designated as an\u00a0outlier. Compute its neighborhood to determine if it\u2019s a\u00a0core point. If yes, start a cluster around this point. If no, label the point as an\u00a0outlier.\nOnce we find a\u00a0core point\u00a0and thus a cluster, expand the cluster by adding all\u00a0directly-reachable\u00a0points to the cluster. Perform \u201cneighborhood jumps\u201d to find all\u00a0density-reachable\u00a0points and add them to the cluster. If an an\u00a0outlier\u00a0is added, change that point\u2019s status from\u00a0outlier\u00a0to\u00a0border point.\nRepeat these two steps until all points are either assigned to a cluster or designated as an\u00a0outlier.\n.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 104,
      "document_type": "slides"
    }
  },
  {
    "content": "Below is the DBSCAN clustering algorithm in pseudocode:\nDBSCAN(dataset, eps, MinPts){\n# cluster index\nC = 1\nfor each unvisited point p in dataset {\n         mark p as visited\n         # find neighbors\n         Neighbors N = find the neighboring points of p\n\n         if |N|>=MinPts:\n             N = N U N'\n             if p' is not a member of any cluster:\n                 add p' to cluster C \n}",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 105,
      "document_type": "slides"
    }
  },
  {
    "content": "CROSS VALIDATION:\nCross-validation is a technique in which we train our model using the subset of the data-set and then evaluate using the complementary subset of the data-set.\nThe three steps involved in cross-validation are as follows :\nSplit data set into training and test set\nUsing the training set train the model.\nTest the model using the test set \nUSE: To get good out of sample accuracy\n\n\n\nEven though we use cross validation technique we get variation  in accuracy when we train our model for that we use K-fold cross validation technique",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 106,
      "document_type": "slides"
    }
  },
  {
    "content": "In K-fold cross validation, we split the data-set into k number of subsets(known as folds) then we perform training on the all the subsets but leave one(k-1) subset for the evaluation of the trained model. In this method, we iterate k times with a different subset reserved for testing purpose each time.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 107,
      "document_type": "slides"
    }
  },
  {
    "content": "Code in python for k-cross validation:\n\nfrom sklearn.model_selection import cross_val_score\nList=cross_val_score(estimator=#name of your model object ,X=#your trained input,y=#correct output,cv=#number of folds(k))\n\nFirst line is importing k fold cross validation function from model_selection sublibrary from sklearn library\n\nsecond line will give a list of accuracies based on k value. we need to average them to get the model accurate accuracy",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 108,
      "document_type": "slides"
    }
  },
  {
    "content": "How to choose the optimal values for the hyperparameters ?\n\nHyperparameters, are the parameters that cannot be directly learned from the regular training process. They are usually fixed before the actual training process begins. These parameters express important properties of the model such as its complexity or how fast it should learn.\nExamples:\nThe k in k-nearest neighbours.\n\nModels can have many hyperparameters and finding the best combination of parameters can be treated as a search problem. One of the best strategies for Hyperparameter tuning is grid_search.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 109,
      "document_type": "slides"
    }
  },
  {
    "content": "GridSearchCV:\n\nIn GridSearchCV approach, machine learning model is evaluated for a range of hyperparameter values. This approach is called GridSearchCV, it searches for best set of hyperparameters from a grid of hyperparameters values.\n\nFor example, if we want to set hyperparameter   K-nearest neighbours model, with different set of values. The gridsearch technique will check model with all possible combinations of hyperparameters, and will return the best one.\nFrom graph we can say best value for k is 10 and grid search will search all the values of k that we given in range and return  the best one",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 110,
      "document_type": "slides"
    }
  },
  {
    "content": "Code in python for getting optimal hyperparameter using gridsearch for support vector machine: \n#importing svm from svc library\nfrom sklearn.svm import SVC\nClassifier=SVC()\n#To import gridsearcv class from sklearn library\nfrom sklearn.model_selection import GridSearchCV\n#creating a list of dictonarties that need to be inputed for grid search\nparameters=[{'C':[1,10,100,1000],'kernel':['linear']},{'C':[1,10,100,1000],'kernel':['rbf'],'gamma':[0.5,0.1,0.01,0.001]}]\n#creating grid search object\ngridsearch=GridSearchCV(estimator=classifier,param_grid=parameters,scoring='accuracy',cv=10,n_jobs=-1)\n#fitting gridsearch with data set\ngd=gridsearch.fit(X_train,y_train)",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 111,
      "document_type": "slides"
    }
  },
  {
    "content": "#fitting gridsearch with data set\ngd=gridsearch.fit(X_train,y_train)\n#best score among all models in grid search\nbestaccuracy=gd.best_score_\n#return the parameters of best model\nbest_param=gd.best_params_",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 112,
      "document_type": "slides"
    }
  },
  {
    "content": "XGBOOST\nXGBoost is an implementation of gradient boosted decision trees designed for speed and performance\nIn this algorithm, decision trees are created in sequential form. Weights play an important role in XGBoost. Weights are assigned to all the independent variables which are then fed into the decision tree which predicts results.",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 113,
      "document_type": "slides"
    }
  },
  {
    "content": "Weight of variables predicted wrong by the tree is increased and these the variables are then fed to the second decision tree. These individual classifiers/predictors then ensemble to give a strong and more precise model. It can work on regression, classification, ranking, and user-defined prediction problems.\n\nFinal value of the classifier is the class which is repeated more number of times among all classifier for classification problem and for regression problem final value is the average of all the regressor values got in sequential trees",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 114,
      "document_type": "slides"
    }
  },
  {
    "content": "Code in python for XGBOOST\n\n#Fitting XGBoost to the training data for classifier\nImport xgboost as xgb\nmy_model=xgb.XGBClassifier()\nmy_model.fit(X_train,y_train)\n\n#predicting the test set results\nY_pred=my_model.predict(X_test)",
    "metadata": {
      "source": "sample.pptx",
      "slide_number": 115,
      "document_type": "slides"
    }
  }
]